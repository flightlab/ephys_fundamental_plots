[["index.html", "Fundamental plots for electrophysiological data Chapter 1 About Citation License", " Fundamental plots for electrophysiological data Vikram B. Baliga 2023-02-27 Chapter 1 About This site is a work in progress. Ultimately, it will provide a walkthrough on how to produce fundamental plots from electrophysiological data. The content was initialized using a bookdown template; accordingly, as this site remains in a developmental stage, content from the template may linger. The original content written here is intended to instruct trainees in the Altshuler Lab at the University of British Columbia to take raw recorded data from electrophysiological examinations and then produce preliminary plots that help characterize the recorded neural spike data. To get started, please use the left navigation and work through chapters in order.  Citation TBD License The content of this work is licensed under CC-BY. For details please see this web page or the LICENSE file in flightlab/ephys_fundamental_plots. "],["preface.html", "Chapter 2 Preface 2.1 R packages &amp; versioning 2.2 %not_in%", " Chapter 2 Preface 2.1 R packages &amp; versioning The R packages listed below will be necessary at some point over the course of this book. I recommend installing them all now. The block of code below is designed to first check if each of the listed packages is already installed on your computer. If any is missing, then an attempt is made to install it from CRAN. Finally, all of the packages are loaded into the environment. ## Specify the packages you&#39;ll use in the script packages &lt;- c(&quot;tidyverse&quot;, &quot;readxl&quot;, &quot;zoo&quot;, &quot;gridExtra&quot;, &quot;R.matlab&quot;, &quot;cowplot&quot;, &quot;easystats&quot;, &quot;circular&quot;, &quot;splines&quot;, &quot;MESS&quot;, ## area under curve &quot;zoo&quot; ## rolling means ) ## Now for each package listed, first check to see if the package is already ## installed. If it is installed, it&#39;s simply loaded. If not, it&#39;s downloaded ## from CRAN and then installed and loaded. package.check &lt;- lapply(packages, FUN = function(x) { if (!require(x, character.only = TRUE)) { install.packages(x, dependencies = TRUE) library(x, character.only = TRUE) } } ) I will use the sessionInfo() command to detail the specific versions of packages I am using (along with other information about my R session). Please note that I am not suggesting you obtain exactly the same version of each package listed below. Instead, the information below is meant to help you assess whether package versioning underlies any trouble you may encounter. ## R version 4.2.0 (2022-04-22 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## attached base packages: ## [1] splines stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] MESS_0.5.9 circular_0.4-95 report_0.5.5 see_0.7.2 ## [5] correlation_0.8.2 modelbased_0.8.5 effectsize_0.7.0.5 parameters_0.18.2 ## [9] performance_0.9.2 bayestestR_0.12.1 datawizard_0.6.0 insight_0.18.2 ## [13] easystats_0.4.3 cowplot_1.1.1 R.matlab_3.6.2 gridExtra_2.3 ## [17] zoo_1.8-10 readxl_1.4.0 forcats_0.5.1 stringr_1.4.0 ## [21] dplyr_1.0.9 purrr_0.3.4 readr_2.1.2 tidyr_1.2.0 ## [25] tibble_3.1.7 ggplot2_3.4.0 tidyverse_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] fs_1.5.2 lubridate_1.8.0 httr_1.4.3 ## [4] tools_4.2.0 backports_1.4.1 bslib_0.3.1 ## [7] utf8_1.2.2 R6_2.5.1 DBI_1.1.2 ## [10] colorspace_2.0-3 withr_2.5.0 tidyselect_1.1.2 ## [13] emmeans_1.7.5 compiler_4.2.0 cli_3.4.1 ## [16] rvest_1.0.2 xml2_1.3.3 sandwich_3.0-2 ## [19] bookdown_0.32 sass_0.4.1 mosaicCore_0.9.0 ## [22] scales_1.2.0 mvtnorm_1.1-3 ggridges_0.5.3 ## [25] digest_0.6.29 ggformula_0.10.1 rmarkdown_2.14.2 ## [28] R.utils_2.12.0 pkgconfig_2.0.3 htmltools_0.5.2 ## [31] labelled_2.9.1 geeM_0.10.1 dbplyr_2.2.0 ## [34] fastmap_1.1.0 rlang_1.0.6 rstudioapi_0.13 ## [37] farver_2.1.0 jquerylib_0.1.4 generics_0.1.2 ## [40] jsonlite_1.8.0 R.oo_1.25.0 googlesheets4_1.0.0 ## [43] magrittr_2.0.3 Matrix_1.4-1 Rcpp_1.0.8.3 ## [46] munsell_0.5.0 fansi_1.0.3 geepack_1.3.4 ## [49] lifecycle_1.0.3 R.methodsS3_1.8.2 stringi_1.7.6 ## [52] multcomp_1.4-19 yaml_2.3.5 MASS_7.3-56 ## [55] ggstance_0.3.5 plyr_1.8.7 grid_4.2.0 ## [58] crayon_1.5.1 lattice_0.20-45 haven_2.5.0 ## [61] hms_1.1.1 knitr_1.39 pillar_1.7.0 ## [64] boot_1.3-28 estimability_1.3 codetools_0.2-18 ## [67] reprex_2.0.1 glue_1.6.2 evaluate_0.15 ## [70] modelr_0.1.8 tweenr_1.0.2 vctrs_0.5.0 ## [73] tzdb_0.3.0 cellranger_1.1.0 polyclip_1.10-0 ## [76] gtable_0.3.0 assertthat_0.2.1 ggforce_0.3.3 ## [79] xfun_0.31 xtable_1.8-4 broom_0.8.0 ## [82] coda_0.19-4 survival_3.3-1 googledrive_2.0.0 ## [85] gargle_1.2.0 TH.data_1.1-1 ellipsis_0.3.2 2.2 %not_in% This guide will also rely on this handy function, which you should add to your code: `%not_in%` &lt;- Negate(`%in%`) This simple operator allows you to determine if an element does not appear in a target object. "],["spike-sorting.html", "Chapter 3 Spike sorting 3.1 Spike2 3.2 Neuralynx", " Chapter 3 Spike sorting We’ll cover how to spike sort using two programs: 1) Spike2 (written by Tony Lapsanksy), and 2) Neuralynx (written by Eric Press) 3.1 Spike2 Written by Tony Lapsansky, February 24, 2023 Save the Spike2 file Use the name structure YEARMODA_sequence_investigator Save data in the corresponding directory “C:\\InvestigatorName\\ephys\\YEAR-MO-DA” Open Spike2 and open the file Apply a digital high pass filter if needed. Note that if the data were collected with the high pass filter set at greater than 100 Hz (no LFP signal) then skip to step 4. Right click on channel and select FIR Digital Filters… (see Spike 2 help → index Digital Filter for explanation) Under the pull down menu for Filter change from Example low pass filter to Example high pass filter Select the Show Details button in the bottom right Adjust blue slider to shift the colour dots above the slider from red to yellow to green, but use the minimum level to achieve green. Fine adjustments can be made just under the slider. Hit Apply Set Destination to the next available channel (often channel 4) Click Okay Close the filtering window. You are given the option to save the filter. Do not do this. It is important to set the filter each time. (?) Set thresholds for spikes Right click on the filtered channel and select New WaveMark Clear previous templates if any are present. To do so, select the trash can icon within each template. Locate the dashed vertical line, which can be found at time 0 in the main window. This line indicates your cursor position. Move the dashed line through the trace to observe potential spike as determined by the default upper and lower thresholds. Right click the upper bound marker (the upper horizontal dashed line in the WaveMark window) and select move away Identify spikes based on the lower bound. It is usually helpful to zoom in on the x-axis (time) to do this. Set the lower bound so that obvious spikes are included and ambiguous spikes are excluded. Choose template setting Move the cursor to a typical spike. The upper window is a base template. Click and hold on the upper trace and drag it to the first available template window. Click on the button just to the left of the trash can icon (on the top half, upper right of the WaveMark window). This is the parameters dialog button. This opens a template settings window. For the line Maximum amplitude change for a match enter 20. This will allow a spike that fits a template to vary in maximum amplitude by up to 20%. For the line Remove the DC offset before template matching, confirm that the box is checked. Nothing else should need to be changed. Click OK. Spike sorting Back in the WaveMark window, make sure that the box Circular replay is unchecked, and that the box Make Templates is checked. Ensure that the vertical cursor on the main window is at time zero (or the first spike). Hit the play button ▶️, which is called “run forward”. This will take several minutes. Use PCA to delete and merge spike templates Select New Channel on the WaveMark window to place the spike data in the next available channel (typically, Channel 5) Close the WaveMark window. Right click on the sorted channel and select Edit WaveMark Within the WaveMark window, go the pull down menu Analyse and select Principal components. Select OK. This opens a window of all spikes colored by template. Rotate around all three axes to determine if there is one, two, or more clusters. Identify templates that should be deleted and those that should be merged. Delete templates that sparse and peripheral. Delete the template(s) in the WaveMark window by selecting that template’s trash can icon. Merge templates by dragging them into the same window Hit the “reclassify” button in the WaveMark window. Export the spike-sorted data File → Export As Select .mat (Matlab data) Use the same filename and location but with the .mat extension. Hit Save Select Add for All Channels Click Export Click OK (this will take several minutes) 3.2 Neuralynx Written by Eric Press, November 11, 2022 Spike sorting database: Check the column labelled Sorting status to find days of recording that are cued meaning they are ready to be sorted. Recordings are cued for spike sorting once information about the recording has been added to the database. This includes observations from the day’s recording, whether the electrode position was moved from the previous recording, and the stimulus condition for each recording. The recordings are stored at the following location and are named/organized by date and time of recording: Computer/LaCie (D:)/Eric’s data/nlx_recordings Filtering the raw traces (CSCs): Use the NlxCSCFiltering tool on any Windows machine to run a band-pass filter on input CSC files. Choose all the CSC files for a given recording, change the PreAppend field to spfilt, which stands for spike-filtered and adjust the DSP filtering fields to match the image to the right. This selects for frequencies in the raw traces where spikes will be found, but removes low frequency (LFP) and high frequency components of the traces. Nix csc filter Examine the filtered traces: Take a closer look at the filtered traces (Open in Neuraview on any Windows machine) and determine which channels are likely to have isolatable spikes and how many distinct spikes there might be. It helps to keep Neuraview open when setting thresholds in the next step. Spike detection from filtered traces: Use the CSCSpikeExtractor tool on any Windows machine to detect spikes above or below a given µV) threshold. The units displayed in the program will be AdBitVolts which are simply 10.92x from the µV value. Based on the filtered traces, within CSCSpikeExtractor, set the spike extraction properties (Spike Extraction -&gt; Properties OR Ctrl+P) as shown above. The Extraction Value is set to 10.92x the µV you chose by viewing the filtered traces. Press Ctrl+S to extract spikes from the selected file at the desired settings. The resulting file will be placed in the extracted spikes filter on the Desktop. Create subfolders in the recording folder for each threshold and move the extracted spikes at each threshold into the appropriate folder. These spike-detected files will be used for spike sorting in the next step. If it helps with detecting real spike waveforms while eliminating noise, run recordings through spike detection at multiple threshold (positive or negative) such that only all putative neurons are accounted for a minimal noise is detected. Spike extraction properties Spike sorting: Open the extracted spikes in Spikesort3D on either the Neuralynx machine or another Windows machine that has an active SpikeSort3D licence. You can also use TeamViewer to control the Neuralynx machine but this works much better with another Windows machine. Press OK when the feature selection window appears. If you want to select alternate features to display, select them from the list provided. Sometimes it can be helpful to use PCA1 – 3 in isolating neurons but often it makes things more challenging. Using the 3D Plot, examine the clustering of spikes. Follow the image below to aid in interacting with the 3D plot (MB = the scroll wheel button i.e. middle mouse button). You can change the features displayed on each axis with Q/W, A/S, and Z/X respectively. Also, Ctrl+P brings up a window that allows you to change the size and opacity of points on the plot (I find size = 2, alpha = 0.5 works well to improve visual definition of the clusters). If distinct clusters are difficult to see, find the combination of 3 features that produces the most noticeable clustering or the greatest spread of points in the space. The features displayed in the 3D plot are shown at the top left of the plot (i.e. X(3) Height # # # #). Use those features for the next step. Run KlustaKwik (Cluster → Autocluster using KlustaKwik) and select the 3 features that generate the most clearly separable clusters on the 3D view – often, the first 3 (Peak, Valley, Energy) do a decent job. Change the MaxPossibleClusters to 10 before pressing Run. The remaining settings should match the image below. Following calculations, use the Waveform window and the 3D plot to group the distinct clusters into what you believe are waveforms produced by distinct neurons. Use the number keys to highlight distinct clusters and Ctrl+M to merge clusters together. Ctrl+C copies the selected cluster and can be used to split a cluster into 2 if you believe portions of the cluster belong to distinct putative neurons. This step takes some practice. You can use Ctrl+Z to undo only one move. Otherwise, you may need to exit without saving and start again at step 4. Save with Ctrl+S often and click OK to overwrite the file. Once you are satisfied with the waveforms left, note how many there are, and whether it seems possible that some of the groups belong to the same neuron. Consider what you know about excitable membranes to make these decisions. Fill out the Spike Sorting Database with the information used to reach this point. This includes, the threshold(s), # of clusters, # of putative neurons (often 1 less than the # of clusters because it would be a stretch to include the smallest amplitude waveform as a distinct, separable neuron), and any else to note from performing sorting. Save each cluster to its own spike file (File → Save Multiple Spike Files) Open the separate spike files you just created, along with the original filtered trace in Neuraview. Scroll along the recording and examine if the sorting you performed seems believable. Do the spikes in different rows really seem like they’re different in the filtered trace? Do some spikes not seem like real spikes? If anything seems amiss, make the appropriate merges in SpikeSort3D before proceding. Export the relevant data from the sorting. Perform the following: File → Save ASCII Timestamp Files File → Save Multiple Spike Files File → Save ASCII Avg Waveforms Also, save the file itself with Ctrl+S Lastly, bring up all the waveforms together on the waveform plot. Take a screenshot and save it to the folder where the extracted spikes (and now timestamps files) are stored. Moving sorted files to other locations: Once a chunk of recordings have been sorted, copy/paste the entire recording file to Eric’s orange 1TB storage drive (Lacie). Place them in the following folder: Eric's data/sorted_recordings "],["raw-data-and-spike-sorted-traces.html", "Chapter 4 Raw data and spike sorted traces 4.1 Including Plots", " Chapter 4 Raw data and spike sorted traces This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 4.1 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "],["data-wrangling.html", "Chapter 5 Data wrangling 5.1 Import example file and metadata", " Chapter 5 Data wrangling Once data have been spike sorted, we are ready to begin working in R. To get to a point where meaningful preliminary plots can be produced, a few things need to be addressed: Labeling the time series of spike &amp; photodiode data based on the stimulus that appears on screen (i.e., matching the log file to the data file). This includes labeling phases (like “blank”, “stationary”, and “moving”) along with experimental metadata such as SF, TF, and stimulus orientation (direction). Re-organizing the spike &amp; photodiode so that separate replicates of a stimulus run are uniquely labelled and then arranged together. Binning the data into 10- and 100-ms data sets, and then exporting CSVs of the unbinned, 10-ms-binned, and 100-ms-binned data. This will be highly useful for situations where you are handling multiple data files (e.g., from different recording days), in which case it is likely that your machine will not be able to store all objects in RAM. Before proceeding any further, please ensure you have installed and loaded all the necessary R packages as detailed in the Preface section of this guide. 5.1 Import example file and metadata We will use a recently-collected data file and its corresponding metadata file to showcase the fundamentals of wrangling ephys data into a more easily plot-able format. 5.1.1 Identify files to import The following code is based on the assumptions that: 1) Your files are stored in a directory entitled /data 2) The basename of each file (i.e., the name of the file, excluding the file extension) is identical for each set of spike sorted data and corresponding metadata log file (e.g., 04132022_009m.mat and 04132022_009m have the same basename, which is 04132022_009m). ## List all files of each file type csv_files &lt;- list.files(&quot;./data&quot;, pattern = &quot;.csv&quot;, full.names = TRUE) mat_files &lt;- list.files(&quot;./data&quot;, pattern = &quot;.mat&quot;, full.names = TRUE) ## Generate metadata tibbles for each file type csv_file_info &lt;- tibble( csv_files = csv_files, ## Extract the basename by removing the file extension basename = basename(csv_files) %&gt;% str_remove(&quot;.csv&quot;), ## NOTE: PLEASE ADJUST THE FOLLOWING LINE TO BE ABLE TO EXCTRACT OUT THE ## DATE BASED ON YOUR NAMING CONVENTION basedate = basename(csv_files) %&gt;% str_sub(start = 1, end = 12) ) mat_file_info &lt;- tibble( mat_files = mat_files, ## Extract the basename by removing the file extension basename = basename(mat_files) %&gt;% str_remove(&quot;.mat&quot;), ## NOTE: AGAIN, PLEASE ADJUST THE FOLLOWING LINE TO BE ABLE TO EXCTRACT OUT ## THE DATE BASED ON YOUR NAMING CONVENTION basedate = basename(mat_files) %&gt;% str_sub(start = 1, end = 12) ) ## Matchmake between .MAT data and .CSV log files csv_mat_filejoin &lt;- inner_join(csv_file_info, mat_file_info, by = &quot;basename&quot;) %&gt;% ## OPTIONAL STEP: remove any rows where either the .MAT or .CSV is missing drop_na() ## Store a vector of basenames in the environment. This will become useful later base_names &lt;- csv_mat_filejoin$basename ## Your end products from this code block should look something like: csv_mat_filejoin ## # A tibble: 1 × 5 ## csv_files basename basedate.x mat_files basedate.y ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ./data/04132022_009m.csv 04132022_009m 04132022_009 ./data/0413202… 04132022_… ## and: base_names ## [1] &quot;04132022_009m&quot; 5.1.2 Data import and preliminary labeling We will now use the R.matlab package to import the .mat file into R and then label the spike and photodiode time series based on the information in the .csv log file Because .mat files can be large, data import can take several minutes. ## Tidy up how R has been using RAM by running garbage collection gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 2525435 134.9 3985180 212.9 3985180 212.9 ## Vcells 4319039 33.0 10146329 77.5 7076199 54.0 ## Set up empty vectors that will collect sets of replicates that we will be ## splitting up metadata_sets &lt;- NULL meta_splits &lt;- NULL data_splits &lt;- NULL "],["raster-and-mean-spike-rate-plots.html", "Chapter 6 Raster and mean spike rate plots 6.1 Including Plots", " Chapter 6 Raster and mean spike rate plots This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 6.1 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "],["spatiotemporal-tuning.html", "Chapter 7 Spatiotemporal tuning 7.1 Including Plots", " Chapter 7 Spatiotemporal tuning This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 7.1 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "],["histological-verification.html", "Chapter 8 Histological verification 8.1 Including Plots", " Chapter 8 Histological verification This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 8.1 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

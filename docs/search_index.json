[["index.html", "Fundamental plots for electrophysiological data 1 About Citation License", " Fundamental plots for electrophysiological data Vikram B. Baliga 2023-03-03 1 About This site is a work in progress. Ultimately, it will provide a walkthrough on how to produce fundamental plots from electrophysiological data. The content was initialized using a bookdown template; accordingly, as this site remains in a developmental stage, content from the template may linger. The original content written here is intended to instruct trainees in the Altshuler Lab at the University of British Columbia to take raw recorded data from electrophysiological examinations and then produce preliminary plots that help characterize the recorded neural spike data. To get started, please use the left navigation and work through chapters in order.  Citation TBD License The content of this work is licensed under CC-BY. For details please see this web page or the LICENSE file in flightlab/ephys_fundamental_plots. "],["preface.html", "2 Preface 2.1 R packages &amp; versioning 2.2 %not_in%", " 2 Preface 2.1 R packages &amp; versioning The R packages listed below will be necessary at some point over the course of this book. I recommend installing them all now. The block of code below is designed to first check if each of the listed packages is already installed on your computer. If any is missing, then an attempt is made to install it from CRAN. Finally, all of the packages are loaded into the environment. ## Specify the packages you&#39;ll use in the script packages &lt;- c(&quot;tidyverse&quot;, &quot;zoo&quot;, &quot;gridExtra&quot;, &quot;R.matlab&quot;, &quot;cowplot&quot;, &quot;easystats&quot;, &quot;circular&quot;, &quot;splines&quot;, &quot;MESS&quot;, ## area under curve &quot;zoo&quot; ## rolling means ) ## Now for each package listed, first check to see if the package is already ## installed. If it is installed, it&#39;s simply loaded. If not, it&#39;s downloaded ## from CRAN and then installed and loaded. package.check &lt;- lapply(packages, FUN = function(x) { if (!require(x, character.only = TRUE)) { install.packages(x, dependencies = TRUE) library(x, character.only = TRUE) } } ) I will use the sessionInfo() command to detail the specific versions of packages I am using (along with other information about my R session). Please note that I am not suggesting you obtain exactly the same version of each package listed below. Instead, the information below is meant to help you assess whether package versioning underlies any trouble you may encounter. ## R version 4.2.2 (2022-10-31 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## attached base packages: ## [1] splines stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] MESS_0.5.9 circular_0.4-95 see_0.7.4 report_0.5.6 ## [5] parameters_0.20.2 performance_0.10.2 modelbased_0.8.6 insight_0.19.0 ## [9] effectsize_0.8.3 datawizard_0.6.5 correlation_0.8.3 bayestestR_0.13.0 ## [13] easystats_0.6.0 cowplot_1.1.1 R.matlab_3.7.0 gridExtra_2.3 ## [17] zoo_1.8-11 lubridate_1.9.2 forcats_1.0.0 stringr_1.5.0 ## [21] dplyr_1.1.0 purrr_1.0.1 readr_2.1.4 tidyr_1.3.0 ## [25] tibble_3.1.8 ggplot2_3.4.1 tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] sass_0.4.5 jsonlite_1.8.4 R.utils_2.12.2 bslib_0.4.2 ## [5] ggstance_0.3.6 yaml_2.3.7 backports_1.4.1 pillar_1.8.1 ## [9] lattice_0.20-45 glue_1.6.2 digest_0.6.31 polyclip_1.10-4 ## [13] colorspace_2.1-0 htmltools_0.5.4 Matrix_1.5-1 R.oo_1.25.0 ## [17] pkgconfig_2.0.3 labelled_2.10.0 broom_1.0.3 haven_2.5.1 ## [21] bookdown_0.32 xtable_1.8-4 mvtnorm_1.1-3 scales_1.2.1 ## [25] tweenr_2.0.2 ggforce_0.4.1 tzdb_0.3.0 timechange_0.2.0 ## [29] emmeans_1.8.4-1 farver_2.1.1 generics_0.1.3 ellipsis_0.3.2 ## [33] cachem_1.0.7 withr_2.5.0 geepack_1.3.9 cli_3.6.0 ## [37] magrittr_2.0.3 estimability_1.4.1 evaluate_0.20 R.methodsS3_1.8.2 ## [41] fansi_1.0.4 MASS_7.3-58.2 geeM_0.10.1 tools_4.2.2 ## [45] hms_1.1.2 lifecycle_1.0.3 munsell_0.5.0 compiler_4.2.2 ## [49] jquerylib_0.1.4 rlang_1.0.6 ggridges_0.5.4 grid_4.2.2 ## [53] rstudioapi_0.14 mosaicCore_0.9.2.1 rmarkdown_2.20 boot_1.3-28 ## [57] gtable_0.3.1 R6_2.5.1 knitr_1.42 fastmap_1.1.1 ## [61] utf8_1.2.3 ggformula_0.10.2 stringi_1.7.12 Rcpp_1.0.10 ## [65] vctrs_0.5.2 tidyselect_1.2.0 xfun_0.37 coda_0.19-4 2.2 %not_in% This guide will also rely on this handy function, which you should add to your code: `%not_in%` &lt;- Negate(`%in%`) This simple operator allows you to determine if an element does not appear in a target object. "],["spike-sorting.html", "3 Spike sorting 3.1 Spike2 3.2 Neuralynx", " 3 Spike sorting We’ll cover how to spike sort using two programs: 1) Spike2 (written by Tony Lapsanksy), and 2) Neuralynx (written by Eric Press) 3.1 Spike2 Written by Tony Lapsansky, February 24, 2023 Save the Spike2 file Use the name structure YEARMODA_sequence_investigator Save data in the corresponding directory “C:\\InvestigatorName\\ephys\\YEAR-MO-DA” Open Spike2 and open the file Apply a digital high pass filter if needed. Note that if the data were collected with the high pass filter set at greater than 100 Hz (no LFP signal) then skip to step 4. Right click on channel and select FIR Digital Filters… (see Spike 2 help → index Digital Filter for explanation) Under the pull down menu for Filter change from Example low pass filter to Example high pass filter Select the Show Details button in the bottom right Adjust blue slider to shift the colour dots above the slider from red to yellow to green, but use the minimum level to achieve green. Fine adjustments can be made just under the slider. Hit Apply Set Destination to the next available channel (often channel Click Okay Close the filtering window. You are given the option to save the filter. Do not do this. It is important to set the filter each time. (?) Set thresholds for spikes Right click on the filtered channel and select New WaveMark Clear previous templates if any are present. To do so, select the trash can icon within each template. Locate the dashed vertical line, which can be found at time 0 in the main window. This line indicates your cursor position. Move the dashed line through the trace to observe potential spike as determined by the default upper and lower thresholds. Right click the upper bound marker (the upper horizontal dashed line in the WaveMark window) and select move away Identify spikes based on the lower bound. It is usually helpful to zoom in on the x-axis (time) to do this. Set the lower bound so that obvious spikes are included and ambiguous spikes are excluded. Choose template setting Move the cursor to a typical spike. The upper window is a base template. Click and hold on the upper trace and drag it to the first available template window. Click on the button just to the left of the trash can icon (on the top half, upper right of the WaveMark window). This is the “parameters dialog” button. This opens a template settings window. For the line Maximum amplitude change for a match enter 20. This will allow a spike that fits a template to vary in maximum amplitude by up to 20%. For the line Remove the DC offset before template matching, confirm that the box is checked. Nothing else should need to be changed. Click OK. Spike sorting Back in the WaveMark window, make sure that the box Circular replay is unchecked, and that the box Make Templates is checked. Ensure that the vertical cursor on the main window is at time zero (or the first spike). Hit the play button ▶️, which is called “run forward”. This will take several minutes. Use PCA to delete and merge spike templates Select New Channel on the WaveMark window to place the spike data in the next available channel (typically, Channel Close the WaveMark window. Right click on the sorted channel and select Edit WaveMark Within the WaveMark window, go the pull down menu Analyse and select Principal components. Select OK. This opens a window of all spikes colored by template. Rotate around all three axes to determine if there is one, two, or more clusters. Identify templates that should be deleted and those that should be merged. Delete templates that sparse and peripheral. Delete the template(s) in the WaveMark window by selecting that template’s trash can icon. Merge templates by dragging them into the same window Hit the reclassify button in the WaveMark window. Export the spike-sorted data File → Export As Select .mat (matlab data) Use the same filename and location but with the .mat extension. Hit Save Select Add for All Channels Click Export Click OK (this will take several minutes) 3.2 Neuralynx Written by Eric Press, November 11, 2022 Spike sorting database: Check the column labelled Sorting status to find days of recording that are cued meaning they are ready to be sorted. Recordings are cued for spike sorting once information about the recording has been added to the database. This includes observations from the day’s recording, whether the electrode position was moved from the previous recording, and the stimulus condition for each recording. The recordings are stored at the following location and are named/organized by date and time of recording: Computer/LaCie (D:)/Eric’s data/nlx_recordings Filtering the raw traces (CSCs): Use the NlxCSCFiltering tool on any Windows machine to run a band-pass filter on input CSC files. Choose all the CSC files for a given recording, change the PreAppend field to spfilt, which stands for spike-filtered and adjust the DSP filtering fields to match the image to the right. This selects for frequencies in the raw traces where spikes will be found, but removes low frequency (LFP) and high frequency components of the traces. Examine the filtered traces: Take a closer look at the filtered traces (Open in Neuraview on any Windows machine) and determine which channels are likely to have isolatable spikes and how many distinct spikes there might be. It helps to keep Neuraview open when setting thresholds in the next step. Spike detection from filtered traces: Use the CSCSpikeExtractor tool on any Windows machine to detect spikes above or below a given µV) threshold. The units displayed in the program will be AdBitVolts which are simply 10.92x from the µV value. Based on the filtered traces, within CSCSpikeExtractor, set the spike extraction properties (Spike Extraction -&gt; Properties OR Ctrl+P) as shown above. The Extraction Value is set to 10.92x the µV you chose by viewing the filtered traces. Press Ctrl+S to extract spikes from the selected file at the desired settings. The resulting file will be placed in the extracted spikes filter on the Desktop. Create subfolders in the recording folder for each threshold and move the extracted spikes at each threshold into the appropriate folder. These spike-detected files will be used for spike sorting in the next step. If it helps with detecting real spike waveforms while eliminating noise, run recordings through spike detection at multiple threshold (positive or negative) such that only all putative neurons are accounted for a minimal noise is detected. Spike sorting: Open the extracted spikes in Spikesort3D on either the Neuralynx machine or another Windows machine that has an active SpikeSort3D licence. You can also use TeamViewer to control the Neuralynx machine but this works much better with another Windows machine. Press OK when the feature selection window appears. If you want to select alternate features to display, select them from the list provided. Sometimes it can be helpful to use PCA1 – 3 in isolating neurons but often it makes things more challenging. Using the 3D Plot, examine the clustering of spikes. Follow the image below to aid in interacting with the 3D plot (MB = the scroll wheel button i.e. middle mouse button). You can change the features displayed on each axis with Q/W, A/S, and Z/X respectively. Also, Ctrl+P brings up a window that allows you to change the size and opacity of points on the plot (I find size = 2, alpha = 0.5 works well to improve visual definition of the clusters). If distinct clusters are difficult to see, find the combination of 3 features that produces the most noticeable clustering or the greatest spread of points in the space. The features displayed in the 3D plot are shown at the top left of the plot (i.e. X(3) Height # # # #). Use those features for the next step. Run KlustaKwik (Cluster → Autocluster using KlustaKwik) and select the 3 features that generate the most clearly separable clusters on the 3D view – often, the first 3 (Peak, Valley, Energy) do a decent job. Change the MaxPossibleClusters to 10 before pressing Run. The remaining settings should match the image below. Following calculations, use the Waveform window and the 3D plot to group the distinct clusters into what you believe are waveforms produced by distinct neurons. Use the number keys to highlight distinct clusters and Ctrl+M to merge clusters together. Ctrl+C copies the selected cluster and can be used to split a cluster into 2 if you believe portions of the cluster belong to distinct putative neurons. This step takes some practice. You can use Ctrl+Z to undo only one move. Otherwise, you may need to exit without saving and start again at step 4. Save with Ctrl+S often and click OK to overwrite the file. Once you are satisfied with the waveforms left, note how many there are, and whether it seems possible that some of the groups belong to the same neuron. Consider what you know about excitable membranes to make these decisions. Fill out the Spike Sorting Database with the information used to reach this point. This includes, the threshold(s), # of clusters, # of putative neurons (often 1 less than the # of clusters because it would be a stretch to include the smallest amplitude waveform as a distinct, separable neuron), and any else to note from performing sorting. Save each cluster to its own spike file (File → Save Multiple Spike Files) Open the separate spike files you just created, along with the original filtered trace in Neuraview. Scroll along the recording and examine if the sorting you performed seems believable. Do the spikes in different rows really seem like they’re different in the filtered trace? Do some spikes not seem like real spikes? If anything seems amiss, make the appropriate merges in SpikeSort3D before proceding. Export the relevant data from the sorting. Perform the following: File → Save ASCII Timestamp Files File → Save Multiple Spike Files File → Save ASCII Avg Waveforms Also, save the file itself with Ctrl+S Lastly, bring up all the waveforms together on the waveform plot. Take a screenshot and save it to the folder where the extracted spikes (and now timestamps files) are stored. Moving sorted files to other locations: Once a chunk of recordings have been sorted, copy/paste the entire recording file to Eric’s orange 1TB storage drive (Lacie). Place them in the following folder: Eric's data/sorted_recordings "],["raw-data-and-spike-sorted-traces.html", "4 Raw data and spike sorted traces 4.1 Including Plots", " 4 Raw data and spike sorted traces This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 4.1 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "],["data-wrangling.html", "5 Data wrangling 5.1 Import example file and metadata 5.2 Organizing replicates (required) and binning (optional) 5.3 Data export", " 5 Data wrangling Once data have been spike sorted, we are ready to begin working in R. To get to a point where meaningful preliminary plots can be produced, a few things need to be addressed: Labeling the time series of spike &amp; photodiode data based on the stimulus that appears on screen (i.e., matching the log file to the data file). This includes labeling phases (like “blank”, “stationary”, and “moving”) along with experimental metadata such as SF, TF, and stimulus orientation (direction). Re-organizing the spike &amp; photodiode so that separate replicates of a stimulus run are uniquely labelled and then arranged together. Binning the data into 10- and 100-ms data sets, and then exporting CSVs of the unbinned, 10-ms-binned, and 100-ms-binned data. This will be highly useful for situations where you are handling multiple data files (e.g., from different recording days), in which case it is likely that your machine will not be able to store all objects in RAM. Before proceeding any further, please ensure you have installed and loaded all the necessary R packages as detailed in the Preface section of this guide. 5.1 Import example file and metadata We will use a recently-collected data file and its corresponding metadata file to showcase the fundamentals of wrangling ephys data into a more easily plot-able format. 5.1.1 Identify files to import The following code is based on the assumptions that: Your files are stored in a directory entitled /data The basename of each file (i.e., the name of the file, excluding the file extension) is identical for each set of spike sorted data and corresponding metadata log file (e.g., 04132022_009m.mat and 04132022_009m have the same basename, which is 04132022_009m). ## List all files of each file type csv_files &lt;- list.files(&quot;./data&quot;, pattern = &quot;.csv&quot;, full.names = TRUE) mat_files &lt;- list.files(&quot;./data&quot;, pattern = &quot;.mat&quot;, full.names = TRUE) ## Generate metadata tibbles for each file type csv_file_info &lt;- tibble( csv_files = csv_files, ## Extract the basename by removing the file extension basename = basename(csv_files) %&gt;% str_remove(&quot;.csv&quot;), ## NOTE: PLEASE ADJUST THE FOLLOWING LINE TO BE ABLE TO EXCTRACT OUT THE ## DATE BASED ON YOUR NAMING CONVENTION basedate = basename(csv_files) %&gt;% str_sub(start = 1, end = 12) ) mat_file_info &lt;- tibble( mat_files = mat_files, ## Extract the basename by removing the file extension basename = basename(mat_files) %&gt;% str_remove(&quot;.mat&quot;), ## NOTE: AGAIN, PLEASE ADJUST THE FOLLOWING LINE TO BE ABLE TO EXCTRACT OUT ## THE DATE BASED ON YOUR NAMING CONVENTION basedate = basename(mat_files) %&gt;% str_sub(start = 1, end = 12) ) ## Matchmake between .MAT data and .CSV log files csv_mat_filejoin &lt;- inner_join(csv_file_info, mat_file_info, by = &quot;basename&quot;) %&gt;% ## OPTIONAL STEP: remove any rows where either the .MAT or .CSV is missing drop_na() ## Store a vector of basenames in the environment. This will become useful later base_names &lt;- csv_mat_filejoin$basename ## Your end products from this code block should look something like: csv_mat_filejoin ## # A tibble: 1 × 5 ## csv_files basename basedate.x mat_files based…¹ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ./data/04132022_009m.csv 04132022_009m 04132022_009 ./data/04132022_0… 041320… ## # … with abbreviated variable name ¹​basedate.y ## and: base_names ## [1] &quot;04132022_009m&quot; 5.1.2 Data import and preliminary labeling We will now use the R.matlab package to import the .mat file into R and then label the spike and photodiode time series based on the information in the .csv log file Because .mat files can be large, data import can take several minutes. Please see in-line comments for further guidance ## Set up empty vectors that will collect sets of replicates that we will be ## splitting up metadata_sets &lt;- NULL meta_splits &lt;- NULL data_splits &lt;- NULL starttime &lt;- Sys.time() ## Optional, will help you assess run time for (i in 1:nrow(csv_mat_filejoin)) { ## Which file # are we working on? print(i) ## Set up temporary objects in which to eventually write data csv_data_sets &lt;- NULL mat_data_sets &lt;- NULL joined_data_sets &lt;- NULL ## Import the matlab file. This may take some time mat_import &lt;- R.matlab::readMat(csv_mat_filejoin[i,&quot;mat_files&quot;]) ## Read in the corresponding csv log file csv_data_sets[[i]] &lt;- read_csv(as.character(csv_mat_filejoin[i,&quot;csv_files&quot;]), show_col_types = FALSE) %&gt;% ## Rename columns for convenience rename( Spatial_Frequency = `Spatial Frequency`, Temporal_Frequency = `Temporal Frequency` ) ## Set up a `SF_cpd` column that tranlates SFs to cycles per degree csv_data_sets[[i]]$SF_cpd[csv_data_sets[[i]]$Spatial_Frequency == 0.000668] &lt;- 2^-6 csv_data_sets[[i]]$SF_cpd[csv_data_sets[[i]]$Spatial_Frequency == 0.001336] &lt;- 2^-5 csv_data_sets[[i]]$SF_cpd[csv_data_sets[[i]]$Spatial_Frequency == 0.00267] &lt;- 2^-4 csv_data_sets[[i]]$SF_cpd[csv_data_sets[[i]]$Spatial_Frequency == 0.0053] &lt;- 2^-3 csv_data_sets[[i]]$SF_cpd[csv_data_sets[[i]]$Spatial_Frequency == 0.0106] &lt;- 2^-2 csv_data_sets[[i]]$SF_cpd[csv_data_sets[[i]]$Spatial_Frequency == 0.0212] &lt;- 2^-1 ## The log file does not have time = 0, so set up a separate tibble to ## add this info in later. Some of the metadata will just be filler for now. initial &lt;- tibble( Trial = &quot;initialization&quot;, Spatial_Frequency = csv_data_sets[[i]]$Spatial_Frequency[1], SF_cpd = csv_data_sets[[i]]$SF_cpd[1], Temporal_Frequency = csv_data_sets[[i]]$Temporal_Frequency[1], Direction = csv_data_sets[[i]]$Direction[1], Time = 0.000 ) ## Find photodiode ## It is almost always in channel 2, but we should be sure to check before ## extracting automatically photod_default_channel &lt;- mat_import[[stringr::str_which(names(mat_import), &quot;Ch2&quot;)[1]]] if (!photod_default_channel[1][[1]][1] == &quot;waveform&quot;) { warning(&quot;File &quot;, i,&quot;: Photodiode channel identity uncertain&quot;) } ## Find spikes ## Similarly, spikes are almost always in channel 5, but we should check spikes_default_channel &lt;- mat_import[[stringr::str_which(names(mat_import), &quot;Ch5&quot;)[1]]] if(&quot;codes&quot; %not_in% attributes(spikes_default_channel)$dimnames[[1]]) { warning(&quot;File &quot;, i,&quot;: Sorted spikes channel identity uncertain&quot;) } ## If that worked, see if we can automatically determine the &quot;times&quot; and ## &quot;codes&quot; slot numbers times_location &lt;- which(attributes(spikes_default_channel)$dimnames[[1]] == &quot;times&quot;) codes_location &lt;- which(attributes(spikes_default_channel)$dimnames[[1]] == &quot;codes&quot;) ## Find matlab&#39;s stimulus change log stim_change_channel &lt;- mat_import[[stringr::str_which(names(mat_import), &quot;Ch3&quot;)[1]]] ## Each sweep should be 5 secs. We&#39;ll check that the median is 5 ## If this results in an error, then the channel identity could be wrong, or ## there may have been an issue with sweep duration during the recording ## process if(!median(round(diff(stim_change_channel[[5]][,1]),1)) == 5) { warning(&quot;File &quot;, i,&quot;: stim change channel identity uncertain&quot;) } ## Determine when the onset of motion occurred according to matlab first_moving_mat &lt;- stim_change_channel[[5]][,1][1] ## Find the first &quot;moving&quot; phase in the log file first_moving_csv &lt;- csv_data_sets[[i]] %&gt;% filter(Trial == &quot;moving&quot;) %&gt;% select(Time) %&gt;% slice(1) %&gt;% as.numeric() ## Find the first &quot;blank&quot; phase in the log file first_blank &lt;- csv_data_sets[[i]] %&gt;% filter(Trial == &quot;blank&quot;) %&gt;% select(Time) %&gt;% slice(1) %&gt;% as.numeric() ## Compute the difference between these two first_mvbl_diff &lt;- first_moving_csv - first_blank ## Check to see if the final row of the metadata is &quot;moving&quot; and truncate ## if not ## This can effectively be done by truncating after the final &quot;moving&quot; phase max_moving_sweep &lt;- max(which(csv_data_sets[[i]]$Trial == &quot;moving&quot;)) first_csv_tmp &lt;- bind_rows(initial, csv_data_sets[[i]]) %&gt;% ## Add 1 to max moving sweep since we tacked on &quot;initial&quot; in previous step ## Then slice to restrict any extraneous partial sweeps slice_head(n = (max_moving_sweep + 1)) %&gt;% ## Add the first event time to &quot;Time&quot; and subtract first_mvbl_diff (~2 secs) ## What this does is shift the log csv&#39;s time stamping to match the matlab ## file&#39;s stim change channel&#39;s time stamping mutate(Time = Time + first_moving_mat - first_mvbl_diff - first_blank) %&gt;% ## Make character version of Time for joining later ## This will be crucial for _join functions mutate(Time_char = as.character(round(Time,3))) ## Duplicate the initialization for ease of setting T0 inception &lt;- initial %&gt;% mutate(Time_char = as.character(round(Time,3))) inception$Trial[1] &lt;- &quot;inception&quot; ## Bind the initialization rows first_csv &lt;- bind_rows(inception, first_csv_tmp) ## Compute stimulus end times first_csv$Stim_end &lt;- c(first_csv$Time[-1], max(first_csv$Time) + 3) ## Get final time final_time &lt;- first_csv$Stim_end[nrow(first_csv)] ## Extract photodiode data ## First generate a time sequence to match to the photodiode trace Time_vec &lt;- seq( from = 0.0000, by = 1 / 25000, length.out = length(photod_default_channel[9][[1]][, 1]) ) ## The key thing is to get a character version of time from this Time_char_vec &lt;- as.character(round(Time_vec, 3)) ## Grab the photodiode data photod_full &lt;- tibble(Photod = photod_default_channel[9][[1]][, 1]) ## Add numeric time photod_full$Time &lt;- seq( from = 0.0000, by = 1 / 25000, length.out = nrow(photod_full) ) options(scipen = 999) photod_full &lt;- photod_full %&gt;% ## Add the character time add_column(Time_char = Time_char_vec) %&gt;% ## Use the charcter time to define a group group_by(Time_char) %&gt;% ## Then average the photodiode within summarise(Photod = mean(Photod)) %&gt;% ungroup() %&gt;% mutate(Time = round(as.numeric(Time_char), 3)) %&gt;% arrange(Time) %&gt;% filter(Time &lt;= final_time) ## Extract all spike data all_spike_dat &lt;- tibble( Time = spikes_default_channel[times_location][[1]][, 1], code = spikes_default_channel[codes_location][[1]][, 1]) %&gt;% ## Characterize time, for purposes of joining later mutate(Time_char = as.character(round(Time, 3))) ## How many distinct neurons are there? n_cells &lt;- sort(unique(all_spike_dat$code)) if(length(n_cells) &gt; 1) { ## if there&#39;s more than one distinct neuron all_spike_dat_tmp &lt;- all_spike_dat %&gt;% ## Group by identity of spiking neuron group_by(code) %&gt;% ## Split into separate dfs, one per neuron group_split() ## Additional cells are labeled as &quot;Spike_n&quot; all_cells &lt;- NULL for (j in n_cells) { #print(j) new_name = paste0(&quot;Spikes_&quot;, j) all_cells[[j+1]] &lt;- all_spike_dat_tmp[[j+1]] ## Consolidate to 3 decimal places all_cells[[j+1]] &lt;- all_cells[[j+1]] %&gt;% group_by(Time_char) %&gt;% summarise(code = mean(code)) %&gt;% mutate(code = ceiling(code)) %&gt;% ungroup() %&gt;% mutate(Time = round(as.numeric(Time_char), 3)) %&gt;% arrange(Time) %&gt;% filter(Time &lt;= final_time) names(all_cells[[j+1]])[match(&quot;code&quot;, names(all_cells[[j+1]]))] &lt;- new_name ## Replace &quot;j&quot; with 1 to indicate presence/absence of spike rather than ## cell identity all_cells[[j+1]][new_name] &lt;- 1 ## If the identity is 1, replace &quot;Spikes_1&quot; with just &quot;Spikes&quot; if (new_name == &quot;Spikes_1&quot;) { names(all_cells[[j+1]])[match(new_name, names(all_cells[[j+1]]))] &lt;- &quot;Spikes&quot; } } ## Consolidate all_spike_dat &lt;- all_cells %&gt;% ## Tack on additional spike columns reduce(full_join, by = &quot;Time_char&quot;) %&gt;% arrange(Time_char) %&gt;% ## Remove time.n columns but ## Do not remove Time_char select(-contains(&quot;Time.&quot;)) %&gt;% ## Regenerate numeric time mutate( Time = as.numeric(Time_char) ) %&gt;% select(Time, Time_char, Spikes, everything()) %&gt;% filter(Time &lt;= final_time) } else { ## If there&#39;s only 1 neuron all_spike_dat &lt;- all_spike_dat %&gt;% group_by(Time_char) %&gt;% summarise(code = mean(code)) %&gt;% mutate(code = ceiling(code)) %&gt;% ungroup() %&gt;% rename(Spikes = code) %&gt;% mutate(Time = round(as.numeric(Time_char), 3)) %&gt;% arrange(Time) %&gt;% filter(Time &lt;= final_time) %&gt;% select(Time, Time_char, everything()) } options(scipen = 999) mat_data_sets[[i]] &lt;- ## Generate a time sequence from 0 to final_time tibble( Time = seq(from = 0, to = final_time, by = 0.001) ) %&gt;% ## Character-ize it mutate(Time_char = as.character(round(Time, 5))) %&gt;% ## Join in the photodiode data left_join(photod_full, by = &quot;Time_char&quot;) %&gt;% select(-Time.y) %&gt;% rename(Time = Time.x) %&gt;% ## Join in the spike data left_join(all_spike_dat, by = &quot;Time_char&quot;) %&gt;% select(-Time.y) %&gt;% rename(Time = Time.x) %&gt;% filter(Time &lt;= final_time) %&gt;% ## Replace NAs with 0 in the Spike columns only mutate( across(starts_with(&quot;Spikes&quot;), ~replace_na(.x, 0)) ) ## Merge the matlab data with the metadata joined_one_full &lt;- mat_data_sets[[i]] %&gt;% ## Join by the character version of time NOT the numerical!! full_join(first_csv, by = &quot;Time_char&quot;) %&gt;% ## Rename columns for clarity of reference rename(Time_mat = Time.x, Time_csv = Time.y) %&gt;% ## Convert character time to numeric time mutate(Time = round(as.numeric(Time_char), 3)) %&gt;% ## Carry metadata forward mutate( Trial = zoo::na.locf(Trial, type = &quot;locf&quot;), Spatial_Frequency = zoo::na.locf(Spatial_Frequency, type = &quot;locf&quot;), SF_cpd = zoo::na.locf(SF_cpd, type = &quot;locf&quot;), Temporal_Frequency = zoo::na.locf(Temporal_Frequency, type = &quot;locf&quot;), Direction = zoo::na.locf(Direction, type = &quot;locf&quot;), Time_csv = zoo::na.locf(Time_csv, type = &quot;locf&quot;), Stim_end = zoo::na.locf(Stim_end, type = &quot;locf&quot;) ) %&gt;% ## Calculate velocity mutate( Speed = Temporal_Frequency/SF_cpd, Log2_Speed = log2(Speed) ) ## Add info to metadata metadata_one_full &lt;- first_csv %&gt;% mutate( Speed = Temporal_Frequency/SF_cpd, Log2_Speed = log2(Speed), Stim_end_diff = c(0, diff(Stim_end)) ) ## Some quality control checks ## What are the stim time differences? stimtime_diffs &lt;- round(metadata_one_full$Stim_end_diff)[-c(1:2)] ## How many total reps were recorded? stimtime_reps &lt;- length(stimtime_diffs)/3 ## What do we expect the overall structure to look like? stimtime_expectation &lt;- rep(c(1,1,3), stimtime_reps) ## Does reality match our expectations? if (!all(stimtime_diffs == stimtime_expectation)) { ## If you get this, investigate the file further and determine what went ## wrong print(&quot;stimtime issue; investigate&quot;) } ## Sometimes the final sweep gets carried for an indefinite amount of time ## before the investigator manually shuts things off. The following block ## truncates accordingly mark_for_removal &lt;- which(round(metadata_one_full$Stim_end_diff) %not_in% c(1, 3)) if (any(mark_for_removal == 1 | mark_for_removal == 2)) { mark_for_removal &lt;- mark_for_removal[mark_for_removal &gt; 2] } if (length(mark_for_removal) &gt; 0) { metadata_sets[[i]] &lt;- metadata_one_full %&gt;% filter(Time &lt; metadata_one_full[mark_for_removal[1],]$Time) joined_data_sets[[i]] &lt;- joined_one_full %&gt;% filter(Time_mat &lt; metadata_one_full[mark_for_removal[1],]$Time) } else { metadata_sets[[i]] &lt;- metadata_one_full joined_data_sets[[i]] &lt;- joined_one_full } ## Organize the metadata for export in the R environment meta_splits[[i]] &lt;- metadata_sets[[i]] %&gt;% ## Get rid of the non-sweep info filter(!Trial == &quot;inception&quot;) %&gt;% filter(!Trial == &quot;initialization&quot;) %&gt;% ## Group by trial #group_by(Spatial_Frequency, Temporal_Frequency, Direction) %&gt;% ## Split by trial group group_split(Spatial_Frequency, Temporal_Frequency, Direction) data_splits[[i]] &lt;- joined_data_sets[[i]] %&gt;% ## Get rid of the non-sweep info filter(!Trial == &quot;inception&quot;) %&gt;% filter(!Trial == &quot;initialization&quot;) %&gt;% ## Group by trial group_by(Spatial_Frequency, Temporal_Frequency, Direction) %&gt;% ## Split by trial group group_split() ## Do some cleanup so large objects don&#39;t linger in memory rm( first_csv, inception, initial, mat_import, first_csv_tmp, photod_default_channel, spikes_default_channel, photod_full, all_spike_dat, all_spike_dat_tmp, first_cell, all_cells, metadata_one_full, joined_one_full, joined_data_sets, csv_data_sets, mat_data_sets ) message(&quot;File &quot;, i, &quot;: &quot;, csv_mat_filejoin[i,&quot;basename&quot;], &quot; imported&quot;) gc() } ## [1] 1 endtime &lt;- Sys.time() endtime - starttime ## Total elapsed time ## Time difference of 1.728763 mins ## Tidy up how R has been using RAM by running garbage collection gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 4118837 220.0 12026327 642.3 15032908 802.9 ## Vcells 107145364 817.5 460334402 3512.1 717891018 5477.1 ## Name each data set according to the basename of the file names(metadata_sets) &lt;- csv_mat_filejoin$basename #base_names names(meta_splits) &lt;- csv_mat_filejoin$basename #base_names names(data_splits) &lt;- csv_mat_filejoin$basename #base_names ## Get organized lists of stimuli that were used ## This will ultimately be used for arranging data by stimuli in a sensible ## order metadata_combos &lt;- NULL for (i in 1:length(metadata_sets)) { metadata_combos[[i]] &lt;- metadata_sets[[i]] %&gt;% ## Get unique stimulus parameters distinct(SF_cpd, Temporal_Frequency, Speed, Direction) %&gt;% arrange(Direction) %&gt;% ## Sort by SF_cpd (smallest to largest) arrange(desc(SF_cpd)) %&gt;% mutate( ## Set a &quot;plot_order&quot; which provides a running order of stimuli plot_order = 1:length(meta_splits[[i]]), name = paste(Direction, &quot;Deg,&quot;, Speed, &quot;Deg/s&quot;) ) } names(metadata_combos) &lt;- csv_mat_filejoin$basename #base_names What we now have is the following: metadata_sets: contains a list of tibbles (one per imported file), each of which comprises the stimulus log metadata_combos: contains a list of tibbles (one per imported file), each of which comprises the stimulus log re-written in a preferred plotting order meta_splits: a list of lists. The first level of the list corresponds to the file identities. Within each file’s list, there is an additional set of lists, each of which contains a tibble with the log info for a specific stimulus combination. Essentially the same as metadata_sets but split up on a per-stimulus basis. data_splits: another list of lists, arranged in the same hierarchical order as meta_splits. Each tibble herein contains the spike and photodiode data from the matlab file on a per-stimulus basis. Note that since we are only using one example file, each of these lists should only be 1 element long (with the latter two having additional elements within the first element) 5.2 Organizing replicates (required) and binning (optional) It is common to record several different sweeps of a stimulus to collect (somewhat) independent replicates of neural responses. The primary task of this section will be to use the lists from the previous section to gather &amp; label replicates of the same stimulus. A secondary task is to deal with binning, if desired (highly recommended). Depending on the goals of plotting and/or analyses, it may be wise to work with either unbinned spike data or to bin the data at a convenient and sensible interval. I generally choose to work at the following levels: Unbinned Bin size = 10 ms Bin size = 100 ms Important note: Rather than provide separate code for each of these 3 scenarios, I will provide one example. Bin size must be declared beforehand – please pay attention. ## Set bin size here ## Units are in ms (e.g. 10 = 10ms) bin_size = 10 ## 10 or 100 or 1 (1 = &quot;unbinned&quot;) slice_size = NULL slicemin = NULL slicemax = NULL condition = NULL if (bin_size == 10){ slice_size &lt;- 501 slicemin &lt;- 202 slicemax &lt;- 498 condition &lt;- &quot;_binsize10&quot; } else if (bin_size == 100){ slice_size &lt;- 51 slicemin &lt;- 21 slicemax &lt;- 49 condition &lt;- &quot;_binsize100&quot; } else if (bin_size == 1){ slice_size &lt;- NULL slicemin &lt;- NULL slicemax &lt;- NULL condition &lt;- &quot;_unbinned&quot; } else { stop(&quot;bin_size is non-standard&quot;) } all_replicate_data_reorganized &lt;- vector(mode = &quot;list&quot;, length = length(meta_splits)) name_sets &lt;- vector(mode = &quot;list&quot;, length = length(meta_splits)) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 4118730 220.0 12026327 642.3 15032908 802.9 ## Vcells 107136174 817.4 368267522 2809.7 717891018 5477.1 starttime &lt;- Sys.time() for (i in 1:length(meta_splits)){ ## i = file number print(i) ## We&#39;ll need to collect data at a per-stimulus level and on a per-replicate ## level within the per-stimulus level ## &quot;j&quot; will be used to designate a unique stimulus ## We&#39;ll first create an empty object in which to collect stimulus-specific ## data replicate_data_reorganized &lt;- NULL ## For each of j unique stimuli... for (j in 1:length(meta_splits[[i]])) { # j = {direction,speed} ## Isolate the j-th data d &lt;- data_splits[[i]][[j]] ## And the j-th log data m &lt;- meta_splits[[i]][[j]] %&gt;% group_by(Trial) %&gt;% ## Label separate replicates mutate(Replicate = row_number()) ## Extract a stimulus label to a name_set that will be used later name_sets[[i]][[j]] &lt;- paste(m$Direction[1], &quot;Deg,&quot;, m$Speed[1], &quot;Deg/s&quot;) ## Set up a temporary object to deal with per-replicate data replicates_ordered &lt;- NULL ## &quot;k&quot; will be used to designate replicate number for (k in 1:max(m$Replicate)){ tmp &lt;- m %&gt;% filter(Replicate == k) ## If you have a complete replicate (i.e., blank, stationary, moving) if (nrow(tmp) == 3 ) { ## Grab the specific data doot &lt;- d %&gt;% filter(Time &gt;= min(tmp$Time)) %&gt;% filter(Time &lt;= max(tmp$Stim_end)) ## Add bin information doot$bin &lt;- rep(1:ceiling(nrow(doot)/bin_size), each = bin_size)[1:nrow(doot)] if (bin_size == 1) { ## IF YOU ARE NOT BINNING, RUN THIS: replicates_ordered[[k]] &lt;- doot %&gt;% mutate( ## Construct a standardized time within the sweep Time_stand = Time_mat - min(Time_mat), ## When does the sweep begin Time_begin = min(Time_mat), ## When does the sweep end Time_end = max(Time_mat), ## Delineate the end of the blank phase Blank_end = tmp$Stim_end[1] - min(Time_mat), ## Delineate the end of the stationary phase Static_end = tmp$Stim_end[2] - min(Time_mat), ## Label the replicate number Replicate = k ) %&gt;% ## Bring stim info to first few columns select(Speed, SF_cpd, Temporal_Frequency, Direction, everything()) %&gt;% ## Just in case there some hang over filter(Time_stand &gt;= 0) } else { ## IF YOU ARE BINNING, RUN THIS: ## First grab time and meta info time_and_meta &lt;- doot %&gt;% ## WITHIN EACH BIN: group_by(bin) %&gt;% summarise( ## Label the trial Trial = first(Trial), ## Midpoint of bin Time_bin_mid = mean(Time_mat), ## Bin beginning Time_bin_begin = min(Time_mat), ## Bin end Time_bin_end = max(Time_mat), ## Spike rate = sum of spikes divided by elapsed time Spike_rate = sum(Spikes) / (max(Time_mat) - min(Time_mat)), Photod_mean = mean(Photod) ) ## Now deal with Spike_N columns hold_spike_n &lt;- doot %&gt;% select(starts_with(&quot;Spikes_&quot;)) %&gt;% add_column(bin = doot$bin) %&gt;% add_column(Time_mat = doot$Time_mat) %&gt;% group_by(bin) %&gt;% summarise(across(starts_with(&quot;Spikes_&quot;), ~ sum(.x) / (max(Time_mat) - min(Time_mat)))) ## Put them together replicates_ordered[[k]] &lt;- time_and_meta %&gt;% left_join(hold_spike_n, by = &quot;bin&quot;) %&gt;% mutate( ## Add in metadata (following same definitions above) Time_stand = Time_bin_mid - min(Time_bin_mid), Blank_end = tmp$Stim_end[1] - min(Time_bin_mid), Static_end = tmp$Stim_end[2] - min(Time_bin_mid), SF_cpd = m$SF_cpd[1], Temporal_Frequency = m$Temporal_Frequency[1], Speed = m$Speed[1], Direction = m$Direction[1], Replicate = k ) %&gt;% ## Bring stim info to first few columns select(Speed, SF_cpd, Temporal_Frequency, Direction, everything()) %&gt;% ## Just in case there some hang over filter(Time_stand &gt;= 0) %&gt;% filter(bin &lt; slice_size + 1) rm(time_and_meta, hold_spike_n) } } } ## Now insert it within the collector of per-stimulus data replicate_data_reorganized[[j]] &lt;- replicates_ordered %&gt;% bind_rows() ## Now insert it within the overall data collector all_replicate_data_reorganized[[i]][[j]] &lt;- replicate_data_reorganized[[j]] ## Toss out temporary objects and clean up rm(replicates_ordered, d, m, tmp) gc() } } ## [1] 1 endtime &lt;- Sys.time() endtime - starttime ## Time difference of 1.031371 mins gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 4122947 220.2 12026327 642.3 15032908 802.9 ## Vcells 109632467 836.5 294614018 2247.8 717891018 5477.1 for (i in 1:length(all_replicate_data_reorganized)) { for (j in 1:length(all_replicate_data_reorganized[[i]])) { names(all_replicate_data_reorganized[[i]])[[j]] &lt;- name_sets[[i]][[j]] } } names(all_replicate_data_reorganized) &lt;- csv_mat_filejoin$basename #base_names At the end of this process, here is how all_replicate_data_reorganized[[1]] should look: ## How long is this list? length(all_replicate_data_reorganized[[1]]) ## [1] 48 ## This object contains 48 individual tibbles. Here&#39;s an example of one ## pulled at random all_replicate_data_reorganized[[1]][sample(1:48, size = 1)] ## $`270 Deg, 256 Deg/s` ## # A tibble: 3,006 × 17 ## Speed SF_cpd Temporal_F…¹ Direc…² bin Trial Time_…³ Time_…⁴ Time_…⁵ Spike…⁶ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 256 0.0312 8 270 1 blank 15.2 15.2 15.2 0 ## 2 256 0.0312 8 270 2 blank 15.2 15.2 15.2 0 ## 3 256 0.0312 8 270 3 blank 15.2 15.2 15.2 0 ## 4 256 0.0312 8 270 4 blank 15.2 15.2 15.2 0 ## 5 256 0.0312 8 270 5 blank 15.2 15.2 15.2 0 ## 6 256 0.0312 8 270 6 blank 15.2 15.2 15.2 0 ## 7 256 0.0312 8 270 7 blank 15.2 15.2 15.2 111. ## 8 256 0.0312 8 270 8 blank 15.2 15.2 15.2 222. ## 9 256 0.0312 8 270 9 blank 15.2 15.2 15.3 0 ## 10 256 0.0312 8 270 10 blank 15.3 15.3 15.3 0 ## # … with 2,996 more rows, 7 more variables: Photod_mean &lt;dbl&gt;, Spikes_0 &lt;dbl&gt;, ## # Spikes_2 &lt;dbl&gt;, Time_stand &lt;dbl&gt;, Blank_end &lt;dbl&gt;, Static_end &lt;dbl&gt;, ## # Replicate &lt;int&gt;, and abbreviated variable names ¹​Temporal_Frequency, ## # ²​Direction, ³​Time_bin_mid, ⁴​Time_bin_begin, ⁵​Time_bin_end, ⁶​Spike_rate ## To consolidate this, you can use bind_rows() all_replicate_data_reorganized[[1]] %&gt;% bind_rows ## # A tibble: 150,300 × 17 ## Speed SF_cpd Temporal_F…¹ Direc…² bin Trial Time_…³ Time_…⁴ Time_…⁵ Spike…⁶ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1024 0.0156 16 0 1 blank 431. 431. 431. 0 ## 2 1024 0.0156 16 0 2 blank 431. 431. 431. 0 ## 3 1024 0.0156 16 0 3 blank 431. 431. 431. 0 ## 4 1024 0.0156 16 0 4 blank 431. 431. 431. 0 ## 5 1024 0.0156 16 0 5 blank 431. 431. 431. 0 ## 6 1024 0.0156 16 0 6 blank 431. 431. 431. 0 ## 7 1024 0.0156 16 0 7 blank 431. 431. 431. 222. ## 8 1024 0.0156 16 0 8 blank 431. 431. 431. 222. ## 9 1024 0.0156 16 0 9 blank 431. 431. 431. 0 ## 10 1024 0.0156 16 0 10 blank 431. 431. 431. 0 ## # … with 150,290 more rows, 7 more variables: Photod_mean &lt;dbl&gt;, ## # Spikes_0 &lt;dbl&gt;, Spikes_2 &lt;dbl&gt;, Time_stand &lt;dbl&gt;, Blank_end &lt;dbl&gt;, ## # Static_end &lt;dbl&gt;, Replicate &lt;int&gt;, and abbreviated variable names ## # ¹​Temporal_Frequency, ²​Direction, ³​Time_bin_mid, ⁴​Time_bin_begin, ## # ⁵​Time_bin_end, ⁶​Spike_rate Bear in mind that I am specifically describing all_replicate_data_reorganized[[1]], NOT all_replicate_data_reorganized. This is because all_replicate_data_reorganized will itself be a list that is as long as the number of distinct data files you are feeding into all of the above. Since there is only 1 example file, all_replicate_data_reorganized is a list that is only 1 entry long at its top level, and within that list is set of 48 lists (one per distinct stimulus), each of which contains a stim-specific tibble of data. 5.3 Data export It is highly recommended that you export all_replicate_data_reorganized. I generally export all_replicate_data_reorganized as a separate csv file for each of the following conditions: Unbinned Bin size = 10 ms Bin size = 100 ms This section will provide code to export each of the above, assuming you used those bin sizes in the previous section. Please be sure to change file naming conventions and other parameters in the event you chose a different bin_size in the previous section. ## Declare export destination export_path &lt;- &quot;./data/&quot; ## The &quot;condition&quot; will be appended to the file name. ## Export each tibble within all_replicate_data_reorganized for (i in 1:length(all_replicate_data_reorganized)) { print(i) dat &lt;- all_replicate_data_reorganized[[i]] %&gt;% bind_rows() write_csv( dat, file = paste0( export_path, names(all_replicate_data_reorganized)[i], condition, &quot;.csv&quot; ) ) rm(dat) } Re-run the above chunk of code per condition you seek to export. For me, this process creates 3 files: 04132022_009m_unbinned.csv, 04132022_009m_binsize10.csv, and 04132022_009m_binsize100.csv.  "],["raster-and-mean-spike-rate-plots.html", "6 Raster and mean spike rate plots 6.1 Data sets 6.2 Raster plot 6.3 Mean spike rate plots", " 6 Raster and mean spike rate plots 6.1 Data sets This section will rely on some of the unbinned and binned data we exported in final steps of the previous section. We’ll start by loading in information ## File paths and basenames of _unbinned.csv files unbinned_filelist &lt;- list.files(&quot;./data/&quot;, pattern = &quot;_unbinned.csv&quot;, full.names = TRUE) unbinned_basenames &lt;- unbinned_filelist %&gt;% str_remove(&quot;./data/&quot;) %&gt;% str_remove(&quot;_unbinned.csv&quot;) ## File paths and basenames of _binsize10.csv files bin10_filelist &lt;- list.files(&quot;./data/&quot;, pattern = &quot;_binsize10.csv&quot;, full.names = TRUE) bin10_basenames &lt;- bin10_filelist %&gt;% str_remove(&quot;./data/&quot;) %&gt;% str_remove(&quot;_binsize10.csv&quot;) ## File paths and basenames of _binsize100.csv files bin100_filelist &lt;- list.files(&quot;./data/&quot;, pattern = &quot;_binsize100.csv&quot;, full.names = TRUE) bin100_basenames &lt;- bin100_filelist %&gt;% str_remove(&quot;./data/&quot;) %&gt;% str_remove(&quot;_binsize100.csv&quot;) You should have something similar to the following: unbinned_filelist; unbinned_basenames; bin10_filelist; bin10_basenames; bin100_filelist; bin100_basenames ## [1] &quot;./data/04132022_009m_unbinned.csv&quot; ## [1] &quot;04132022_009m&quot; ## [1] &quot;./data/04132022_009m_binsize10.csv&quot; ## [1] &quot;04132022_009m&quot; ## [1] &quot;./data/04132022_009m_binsize100.csv&quot; ## [1] &quot;04132022_009m&quot; 6.2 Raster plot Here is an example of a raster plot, using the wrangled data generated in the previous section. This type of plot shows the timing of spike events within each replicate sweep, and for our purposes, we’ll produce a view of this for each of the various stimulus conditions It is important to note that we will need unbinned data for this. This is because a raster plot shows discrete spiking events through the course of a time sweep. ## Read in unbinned data unbinned_data &lt;- read_csv(&quot;./data/04132022_009m_unbinned.csv&quot;, show_col_types = FALSE) %&gt;% as_tibble() ## Generate the code for the ggplot and save it as &quot;plot&quot; rasterplot &lt;- unbinned_data %&gt;% ## Remove any rows where spiking does not occur in the Spikes column filter(Spikes == 1) %&gt;% ## Convert Trial and Speed into factors and specify their level ordering ## This will make it easier to get the subplots in the order we want them mutate(Trial = factor(Trial, levels = c(&quot;blank&quot;, &quot;stationary&quot;, &quot;moving&quot;)), Speed = factor(Speed, levels = c(1024, 256, 32, 4, 0.5, 0.062))) %&gt;% ggplot(aes(x = Time_stand, y = Replicate)) + ## The next three blocks will undershade each subplot according to stimulus ## phase (i.e., blank, stationary, moving) annotate(&quot;rect&quot;, xmin = 0, xmax = first(unbinned_data$Blank_end), ymin = 0.5, ymax = 10.5, alpha = 0.1, color = NA, fill = &quot;red&quot;) + annotate(&quot;rect&quot;, xmin = first(unbinned_data$Blank_end), xmax = first(unbinned_data$Static_end), ymin = 0.5, ymax = 10.5, alpha = 0.1, color = NA, fill = &quot;darkgoldenrod1&quot;) + annotate(&quot;rect&quot;, xmin = first(unbinned_data$Static_end), xmax = 5, ymin = 0.5, ymax = 10.5, alpha = 0.1, color = NA, fill = &quot;forestgreen&quot;) + ## Up to 10 replicates were used, so we will force the y-axis to go to 10 scale_y_continuous( limits = c(0.5, 10.5), expand = c(0, 0), breaks = c(5, 10) ) + ## There are multiple ways to plot a spike event. Since 100% of the rows in ## this filtered data set are spike events, we can simply plot a symbol at ## each time (Time_stand) that appears in the data. The `|` symbol is a good ## choice. geom_point(pch = &#39;|&#39;, size = 1.5) + xlab(&quot;Time (sec)&quot;) + ggtitle(paste0(&quot;04132022_009m raster&quot;)) + ## Use facet_grid() to create a grid of subplots. Rows will correspond to ## Speeds, and columns correspond to Directions facet_grid(rows = vars(Speed), cols = vars(Direction)) + theme_classic() + theme(legend.position = &#39;none&#39;, panel.spacing = unit(0.1, &quot;lines&quot;)) rasterplot 6.2.1 Export to PDF Should you elect to export this plot by itself to a PDF, you can do the following: ## Should you elect to export this as a PDF pdf( file = &quot;./path/to/directory/filename.pdf&quot;, width = 10.5, height = 8, title = &quot;04132022_009m raster&quot;, paper = &quot;USr&quot;, bg = &quot;white&quot;, pagecentre = TRUE, colormodel = &quot;srgb&quot; ) plot(rasterplot) dev.off() 6.3 Mean spike rate plots "],["direction-tuning.html", "7 Direction tuning 7.1 Including Plots", " 7 Direction tuning This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 7.1 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "],["spatiotemporal-tuning.html", "8 Spatiotemporal tuning 8.1 Including Plots", " 8 Spatiotemporal tuning This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 8.1 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "],["histological-verification.html", "9 Histological verification 9.1 Including Plots", " 9 Histological verification This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 9.1 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

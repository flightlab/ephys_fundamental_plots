[["index.html", "Fundamental plots for electrophysiological data 1 About Citation License", " Fundamental plots for electrophysiological data Vikram B. Baliga 2023-04-05 1 About Until this statement is deleted from this page, please consider everything you see here a work in progress. Ultimately, this site will provide a walkthrough on how to produce fundamental plots from electrophysiological data. The content was initialized using a bookdown template; accordingly, as this site remains in a developmental stage, content from the template may linger. The original content written here is intended to instruct trainees in the Altshuler Lab at the University of British Columbia to take raw recorded data from electrophysiological examinations and then produce preliminary plots that help characterize the recorded neural spike data. To get started, please use the left navigation and work through chapters in order.  Citation TBD License The content of this work is licensed under CC-BY. For details please see this web page or the LICENSE file in flightlab/ephys_fundamental_plots. "],["preface.html", "2 Preface 2.1 R packages &amp; versioning 2.2 %not_in%", " 2 Preface 2.1 R packages &amp; versioning The R packages listed below will be necessary at some point over the course of this book. I recommend installing them all now. The block of code below is designed to first check if each of the listed packages is already installed on your computer. If any is missing, then an attempt is made to install it from CRAN. Finally, all of the packages are loaded into the environment. ## Specify the packages you&#39;ll use in the script packages &lt;- c(&quot;tidyverse&quot;, &quot;zoo&quot;, &quot;gridExtra&quot;, &quot;R.matlab&quot;, &quot;cowplot&quot;, &quot;easystats&quot;, &quot;circular&quot;, &quot;splines&quot;, &quot;MESS&quot;, ## area under curve &quot;zoo&quot; ## rolling means ) ## Now for each package listed, first check to see if the package is already ## installed. If it is installed, it&#39;s simply loaded. If not, it&#39;s downloaded ## from CRAN and then installed and loaded. package.check &lt;- lapply(packages, FUN = function(x) { if (!require(x, character.only = TRUE)) { install.packages(x, dependencies = TRUE) library(x, character.only = TRUE) } } ) I will use the sessionInfo() command to detail the specific versions of packages I am using (along with other information about my R session). Please note that I am not suggesting you obtain exactly the same version of each package listed below. Instead, the information below is meant to help you assess whether package versioning underlies any trouble you may encounter. ## R version 4.2.2 (2022-10-31 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_United States.utf8 ## [2] LC_CTYPE=English_United States.utf8 ## [3] LC_MONETARY=English_United States.utf8 ## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.utf8 ## ## attached base packages: ## [1] splines stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] MESS_0.5.9 circular_0.4-95 see_0.7.4 report_0.5.6 ## [5] parameters_0.20.2 performance_0.10.2 modelbased_0.8.6 insight_0.19.0 ## [9] effectsize_0.8.3 datawizard_0.6.5 correlation_0.8.3 bayestestR_0.13.0 ## [13] easystats_0.6.0 cowplot_1.1.1 R.matlab_3.7.0 gridExtra_2.3 ## [17] zoo_1.8-11 lubridate_1.9.2 forcats_1.0.0 stringr_1.5.0 ## [21] dplyr_1.1.0 purrr_1.0.1 readr_2.1.4 tidyr_1.3.0 ## [25] tibble_3.1.8 ggplot2_3.4.1 tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] sass_0.4.5 jsonlite_1.8.4 R.utils_2.12.2 bslib_0.4.2 ## [5] ggstance_0.3.6 yaml_2.3.7 backports_1.4.1 pillar_1.8.1 ## [9] lattice_0.20-45 glue_1.6.2 digest_0.6.31 polyclip_1.10-4 ## [13] colorspace_2.1-0 htmltools_0.5.4 Matrix_1.5-1 R.oo_1.25.0 ## [17] pkgconfig_2.0.3 labelled_2.10.0 broom_1.0.3 haven_2.5.1 ## [21] bookdown_0.32 xtable_1.8-4 mvtnorm_1.1-3 scales_1.2.1 ## [25] tweenr_2.0.2 ggforce_0.4.1 tzdb_0.3.0 timechange_0.2.0 ## [29] emmeans_1.8.4-1 farver_2.1.1 generics_0.1.3 ellipsis_0.3.2 ## [33] cachem_1.0.7 withr_2.5.0 geepack_1.3.9 cli_3.6.0 ## [37] magrittr_2.0.3 estimability_1.4.1 evaluate_0.20 R.methodsS3_1.8.2 ## [41] fansi_1.0.4 MASS_7.3-58.2 geeM_0.10.1 tools_4.2.2 ## [45] hms_1.1.2 lifecycle_1.0.3 munsell_0.5.0 compiler_4.2.2 ## [49] jquerylib_0.1.4 rlang_1.0.6 ggridges_0.5.4 grid_4.2.2 ## [53] rstudioapi_0.14 mosaicCore_0.9.2.1 rmarkdown_2.20 boot_1.3-28 ## [57] gtable_0.3.1 R6_2.5.1 knitr_1.42 fastmap_1.1.1 ## [61] utf8_1.2.3 ggformula_0.10.2 stringi_1.7.12 Rcpp_1.0.10 ## [65] vctrs_0.5.2 tidyselect_1.2.0 xfun_0.37 coda_0.19-4 2.2 %not_in% This guide will also rely on this handy function, which you should add to your code: `%not_in%` &lt;- Negate(`%in%`) This simple operator allows you to determine if an element does not appear in a target object. "],["spike-sorting.html", "3 Spike sorting 3.1 Spike2 3.2 Neuralynx", " 3 Spike sorting We’ll cover how to spike sort using two programs: 1) Spike2 (written by Tony Lapsansky) and 2) Neuralynx (written by Eric Press). The function of spike sorting is to isolate action potentials from the background voltage signal. These methods use the shape of the waveform to detect and distinguish the spiking activity of each neuron recorded by an electrode. 3.1 Spike2 Written by Tony Lapsansky, February 24, 2023 These instructions assume that you have been given a Spike2 recording file (extension .smrx) and asked to spike sort. Spike2 includes a detailed description of the program, accessible by clicking Help → Index Spike2 3.1.1 File naming conventions: Use the name structure YEARMODA_sequence_investigator Save data in the corresponding directory “C:\\InvestigatorName\\ephys\\YEAR-MO-DA” 3.1.2 Spike sorting with Spike2 Open the main Spike2 file for the recording. This file should have the extension .smrx. Apply a digital high pass filter, if needed. Note: if the data were collected with the high pass filter set at greater than 100 Hz (no LFP signal) then proceed to step 3. Right click on the raw data channel (typically Ch1) and select FIR Digital Filters…. We want to use an FIR filter rather than an IIR filter as the latter can introduce a variable time lag in the resulting data (see Spike 2 Help → Index → Digital Filter for full explanation). Under the pull down menu for Filter, change the filter from Example low pass filter to Example high pass filter. Select the Show Details button in the bottom right. Adjust blue slider change the filter length. Shift the slider until the coloured dots above the slider from red to yellow to green. This removes wobbles in the data. Use the minimum level (~1019) to achieve green. Fine adjustments can be made just under the slider. Hit Apply Set Destination to the next available channel (typically Channel 4) Click Okay Close the filtering window. You are given the option to save the filter. This is unnecessary. Setting the threshold for spike identification Right click on the filtered channel and select New WaveMark Clear previous templates if any are present. To do so, select the trash can icon within each template. These may be present from a previous session. Locate your cursor position, indicated by the vertical dashed line in the main window (typically found at time 0) Slide the dashed line horizontally through the trace to observe potential spikes as determined by the default upper and lower thresholds. Right click the upper bound marker (the upper horizontal dashed line in the WaveMark window) and select Move Away. We will rely on the lower bound to identify spikes for sorting, as the activity above baseline is typically closer in magnitude to the background. Slide the dashed line horizontally through the trace to observe potential spikes as determined by the lower threshold alone. Adjust the lower threshold to catch spikes of interest. This threshold will vary based based on the distance between the electrode and the neuron, the quality of the isolate, and the level of background noise. Values between 50 mV and 200 mV are typical.Set the lower bound so that spikes of interest are included and ambiguous spikes are excluded. Designing the spike template Move the cursor to a characteristic spike. In the upper window, you will see the provisional template. Click and hold on the trace in the upper window and drag it to the first available spot in the lower, template window. To set parameters for spike sorting, click on the button just to the left of the trash can icon (on the top half, upper right of the WaveMark window). This is the “parameters dialog” button. This opens a template settings window. For the line Maximum amplitude change for a match enter a value between 10 and 20. This will allow a spike that fits a template to vary in maximum amplitude by up to 10-20%. For the line Remove the DC offset before template matching, confirm that the box is checked. This means that Spike2 will account for abrupt shifts in the signal baseline before template matching. This is a stop-gap for any issues with the digital high pass filter. Click OK. Spike sorting Back in the WaveMark window, make sure that the box Circular replay is unchecked. If checked, spike sorting will loop indefinitely. Ensure that the vertical cursor on the main window is at time zero (or the first spike) so that no spikes are missed. Back in the WaveMark window, make sure that the box Make templates is checked. If unchecked, only spikes corresponding to the provisional template will be identified. We want to let spike2 help us to identify potential multi-unit activity. Hit the play button ▶️, which is called “run forward”. Spike sorting will proceed for several minutes. Each identified spike will appear briefly in the WaveMark window and will be assigned to a template. In this image, I have selected options for Overdraw and Show template limits Merge, delete, and save templates After spike sorting has completed, select New Channel on the WaveMark window to place the spike sorted data in the next available channel (typically, Channel 5) Close the existing WaveMark window. Right click on the spike sorted channel and select Edit WaveMark. Within the WaveMark window, go the pull down menu Analyse and select Principal components. Select OK. This opens a window containing a principal component analysis of all spikes colored by their assigned template. Rotate around all three axes to determine if there is one, two, or more clusters. In theory, each cluster corresponds to a single neuron. Often, spikes are categorized into multiple templates, but realistically correspond to the activity of a single neuron. Identify templates that should be deleted and those that should be merged. We will delete spikes corresponding to templates that are sparse and peripheral. Delete the template(s) in the WaveMark window by selecting that template’s trash can icon. Merge templates by dragging them into the same window Hit the reclassify button in the WaveMark window to commit these changes to the data in the main window. In this example, we have good evidence from the PCA to merge these five templates. Export the spike-sorted data File → Export As Select .mat (matlab data) Use the same filename and location but with the .mat extension. Hit Save Select Add for All Channels Click Export Click OK (this will take several minutes) Note: May need to select an earlier MATLAB file convention to work with R. 3.2 Neuralynx Written by Eric Press, November 11, 2022 Spike sorting database: Check the column labelled Sorting status to find days of recording that are cued meaning they are ready to be sorted. Recordings are cued for spike sorting once information about the recording has been added to the database. This includes observations from the day’s recording, whether the electrode position was moved from the previous recording, and the stimulus condition for each recording. The recordings are stored at the following location and are named/organized by date and time of recording: Computer/LaCie (D:)/Eric’s data/nlx_recordings Filtering the raw traces (CSCs): Use the NlxCSCFiltering tool on any Windows machine to run a band-pass filter on input CSC files. Choose all the CSC files for a given recording, change the PreAppend field to spfilt, which stands for spike-filtered and adjust the DSP filtering fields to match the image to the right. This selects for frequencies in the raw traces where spikes will be found, but removes low frequency (LFP) and high frequency components of the traces. Examine the filtered traces: Take a closer look at the filtered traces (Open in Neuraview on any Windows machine) and determine which channels are likely to have isolatable spikes and how many distinct spikes there might be. It helps to keep Neuraview open when setting thresholds in the next step. Spike detection from filtered traces: Use the CSCSpikeExtractor tool on any Windows machine to detect spikes above or below a given µV) threshold. The units displayed in the program will be AdBitVolts which are simply 10.92x from the µV value. Based on the filtered traces, within CSCSpikeExtractor, set the spike extraction properties (Spike Extraction -&gt; Properties OR Ctrl+P) as shown above. The Extraction Value is set to 10.92x the µV you chose by viewing the filtered traces. Press Ctrl+S to extract spikes from the selected file at the desired settings. The resulting file will be placed in the extracted spikes filter on the Desktop. Create subfolders in the recording folder for each threshold and move the extracted spikes at each threshold into the appropriate folder. These spike-detected files will be used for spike sorting in the next step. If it helps with detecting real spike waveforms while eliminating noise, run recordings through spike detection at multiple threshold (positive or negative) such that only all putative neurons are accounted for a minimal noise is detected. Spike sorting: Open the extracted spikes in Spikesort3D on either the Neuralynx machine or another Windows machine that has an active SpikeSort3D licence. You can also use TeamViewer to control the Neuralynx machine but this works much better with another Windows machine. Press OK when the feature selection window appears. If you want to select alternate features to display, select them from the list provided. Sometimes it can be helpful to use PCA1 – 3 in isolating neurons but often it makes things more challenging. Using the 3D Plot, examine the clustering of spikes. Follow the image below to aid in interacting with the 3D plot (MB = the scroll wheel button i.e. middle mouse button). You can change the features displayed on each axis with Q/W, A/S, and Z/X respectively. Also, Ctrl+P brings up a window that allows you to change the size and opacity of points on the plot (I find size = 2, alpha = 0.5 works well to improve visual definition of the clusters). If distinct clusters are difficult to see, find the combination of 3 features that produces the most noticeable clustering or the greatest spread of points in the space. The features displayed in the 3D plot are shown at the top left of the plot (i.e. X(3) Height # # # #). Use those features for the next step. Run KlustaKwik (Cluster → Autocluster using KlustaKwik) and select the 3 features that generate the most clearly separable clusters on the 3D view – often, the first 3 (Peak, Valley, Energy) do a decent job. Change the MaxPossibleClusters to 10 before pressing Run. The remaining settings should match the image below. Following calculations, use the Waveform window and the 3D plot to group the distinct clusters into what you believe are waveforms produced by distinct neurons. Use the number keys to highlight distinct clusters and Ctrl+M to merge clusters together. Ctrl+C copies the selected cluster and can be used to split a cluster into 2 if you believe portions of the cluster belong to distinct putative neurons. This step takes some practice. You can use Ctrl+Z to undo only one move. Otherwise, you may need to exit without saving and start again at step 4. Save with Ctrl+S often and click OK to overwrite the file. Once you are satisfied with the waveforms left, note how many there are, and whether it seems possible that some of the groups belong to the same neuron. Consider what you know about excitable membranes to make these decisions. Fill out the Spike Sorting Database with the information used to reach this point. This includes, the threshold(s), # of clusters, # of putative neurons (often 1 less than the # of clusters because it would be a stretch to include the smallest amplitude waveform as a distinct, separable neuron), and any else to note from performing sorting. Save each cluster to its own spike file (File → Save Multiple Spike Files) Open the separate spike files you just created, along with the original filtered trace in Neuraview. Scroll along the recording and examine if the sorting you performed seems believable. Do the spikes in different rows really seem like they’re different in the filtered trace? Do some spikes not seem like real spikes? If anything seems amiss, make the appropriate merges in SpikeSort3D before proceding. Export the relevant data from the sorting. Perform the following: File → Save ASCII Timestamp Files File → Save Multiple Spike Files File → Save ASCII Avg Waveforms Also, save the file itself with Ctrl+S Lastly, bring up all the waveforms together on the waveform plot. Take a screenshot and save it to the folder where the extracted spikes (and now timestamps files) are stored. Moving sorted files to other locations: Once a chunk of recordings have been sorted, copy/paste the entire recording file to Eric’s orange 1TB storage drive (Lacie). Place them in the following folder: Eric's data/sorted_recordings "],["quick-version---condensed-plots.html", "4 Quick version - condensed plots", " 4 Quick version - condensed plots Coming soon "],["data-wrangling.html", "5 Data wrangling 5.1 Import example file and metadata 5.2 Organizing replicates (required) and binning (optional) 5.3 Data export", " 5 Data wrangling Once data have been spike sorted, we are ready to begin working in R. To get to a point where meaningful preliminary plots can be produced, a few things need to be addressed: Labeling the time series of spike &amp; photodiode data based on the stimulus that appears on screen (i.e., matching the log file to the data file). This includes labeling phases (like “blank”, “stationary”, and “moving”) along with experimental metadata such as SF, TF, and stimulus orientation (direction). Re-organizing the spike &amp; photodiode so that separate replicates of a stimulus run are uniquely labelled and then arranged together. Binning the data into 10- and 100-ms data sets, and then exporting CSVs of the unbinned, 10-ms-binned, and 100-ms-binned data. This will be highly useful for situations where you are handling multiple data files (e.g., from different recording days), in which case it is likely that your machine will not be able to store all objects in RAM. Before proceeding any further, please ensure you have installed and loaded all the necessary R packages as detailed in the Preface section of this guide. 5.1 Import example file and metadata We will use a recently-collected data file and its corresponding metadata file to showcase the fundamentals of wrangling ephys data into a more easily plot-able format. 5.1.1 Identify files to import The following code is based on the assumptions that: Your files are stored in a directory entitled /data The basename of each file (i.e., the name of the file, excluding the file extension) is identical for each set of spike sorted data and corresponding metadata log file (e.g., 2023-02-16_001.mat and 2023-02-16_001.csv have the same basename, which is 2023-02-16_001). ## List all files of each file type csv_files &lt;- list.files(&quot;./data&quot;, pattern = &quot;.csv&quot;, full.names = TRUE) mat_files &lt;- list.files(&quot;./data&quot;, pattern = &quot;.mat&quot;, full.names = TRUE) ## Generate metadata tibbles for each file type csv_file_info &lt;- tibble( csv_files = csv_files, ## Extract the basename by removing the file extension basename = basename(csv_files) %&gt;% str_remove(&quot;.csv&quot;), ## NOTE: PLEASE ADJUST THE FOLLOWING LINE TO BE ABLE TO EXCTRACT OUT THE ## DATE BASED ON YOUR NAMING CONVENTION basedate = basename(csv_files) %&gt;% str_sub(start = 1, end = 12) ) mat_file_info &lt;- tibble( mat_files = mat_files, ## Extract the basename by removing the file extension basename = basename(mat_files) %&gt;% str_remove(&quot;.mat&quot;), ## NOTE: AGAIN, PLEASE ADJUST THE FOLLOWING LINE TO BE ABLE TO EXCTRACT OUT ## THE DATE BASED ON YOUR NAMING CONVENTION basedate = basename(mat_files) %&gt;% str_sub(start = 1, end = 12) ) ## Matchmake between .MAT data and .CSV log files csv_mat_filejoin &lt;- inner_join(csv_file_info, mat_file_info, by = &quot;basename&quot;) %&gt;% ## OPTIONAL STEP: remove any rows where either the .MAT or .CSV is missing drop_na() ## Store a vector of basenames in the environment. This will become useful later base_names &lt;- csv_mat_filejoin$basename ## Your end products from this code block should look something like: csv_mat_filejoin ## # A tibble: 1 × 5 ## csv_files basename basedate.x mat_files based…¹ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ./data/2023-02-16_001.csv 2023-02-16_001 2023-02-16_0 ./data/2023-02-… 2023-0… ## # … with abbreviated variable name ¹​basedate.y ## and: base_names ## [1] &quot;2023-02-16_001&quot; 5.1.2 Data import and preliminary labeling We will now use the R.matlab package to import the .mat file into R and then label the spike and photodiode time series based on the information in the .csv log file Because .mat files can be large, data import can take several minutes. Please see in-line comments for further guidance ## Set up empty vectors that will collect sets of replicates that we will be ## splitting up metadata_sets &lt;- NULL meta_splits &lt;- NULL data_splits &lt;- NULL gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 2454829 131.2 3878522 207.2 3878522 207.2 ## Vcells 4234160 32.4 10146329 77.5 6629589 50.6 starttime &lt;- Sys.time() ## Optional, will help you assess run time for (i in 1:nrow(csv_mat_filejoin)) { ## Which file # are we working on? print(i) ## Set up temporary objects in which to eventually write data csv_data_sets &lt;- NULL mat_data_sets &lt;- NULL joined_data_sets &lt;- NULL ## Import the matlab file. This may take some time mat_import &lt;- R.matlab::readMat(csv_mat_filejoin[i,&quot;mat_files&quot;]) ## Read in the corresponding csv log file csv_data_sets[[i]] &lt;- read_csv(as.character(csv_mat_filejoin[i,&quot;csv_files&quot;]), show_col_types = FALSE) %&gt;% ## Rename columns for convenience rename( Spatial_Frequency = `Spatial Frequency`, Temporal_Frequency = `Temporal Frequency`, Cycles_Per_Pixel = `Cycles Per Pixel` ) ## Determine if spatial frequencies need to be transformed sfs &lt;- csv_data_sets[[i]]$Spatial_Frequency %&gt;% unique() %&gt;% sort() cpps &lt;- c(0.000668, 0.001336, 0.002670, 0.005300, 0.010600, 0.021200) if (all(sfs == cpps)) { ## If true, convert to cpd using the following mapping: csv_data_sets[[i]]$Spatial_Frequency[csv_data_sets[[i]]$Spatial_Frequency == 0.000668] &lt;- 2^-6 csv_data_sets[[i]]$Spatial_Frequency[csv_data_sets[[i]]$Spatial_Frequency == 0.001336] &lt;- 2^-5 csv_data_sets[[i]]$Spatial_Frequency[csv_data_sets[[i]]$Spatial_Frequency == 0.00267] &lt;- 2^-4 csv_data_sets[[i]]$Spatial_Frequency[csv_data_sets[[i]]$Spatial_Frequency == 0.0053] &lt;- 2^-3 csv_data_sets[[i]]$Spatial_Frequency[csv_data_sets[[i]]$Spatial_Frequency == 0.0106] &lt;- 2^-2 csv_data_sets[[i]]$Spatial_Frequency[csv_data_sets[[i]]$Spatial_Frequency == 0.0212] &lt;- 2^-1 } ## The log file does not have time = 0, so set up a separate tibble to ## add this info in later. Some of the metadata will just be filler for now. initial &lt;- tibble( Trial = &quot;initialization&quot;, Spatial_Frequency = csv_data_sets[[i]]$Spatial_Frequency[1], Cycles_Per_Pixel = csv_data_sets[[i]]$Cycles_Per_Pixel[1], Temporal_Frequency = csv_data_sets[[i]]$Temporal_Frequency[1], Direction = csv_data_sets[[i]]$Direction[1], Time = 0.000 ) ## Find photodiode ## It is almost always in channel 2, but we should be sure to check before ## extracting automatically photod_default_channel &lt;- mat_import[[stringr::str_which(names(mat_import), &quot;Ch2&quot;)[1]]] if (!photod_default_channel[1][[1]][1] == &quot;waveform&quot;) { warning(&quot;File &quot;, i,&quot;: Photodiode channel identity uncertain&quot;) } ## Find spikes ## Similarly, spikes are almost always in channel 5, but we should check spikes_default_channel &lt;- mat_import[[stringr::str_which(names(mat_import), &quot;Ch5&quot;)[1]]] if(&quot;codes&quot; %not_in% attributes(spikes_default_channel)$dimnames[[1]]) { warning(&quot;File &quot;, i,&quot;: Sorted spikes channel identity uncertain&quot;) } ## If that worked, see if we can automatically determine the &quot;times&quot; and ## &quot;codes&quot; slot numbers times_location &lt;- which(attributes(spikes_default_channel)$dimnames[[1]] == &quot;times&quot;) codes_location &lt;- which(attributes(spikes_default_channel)$dimnames[[1]] == &quot;codes&quot;) ## Find matlab&#39;s stimulus change log stim_change_channel &lt;- mat_import[[stringr::str_which(names(mat_import), &quot;Ch3&quot;)[1]]] ## Each sweep should be 5 secs. We&#39;ll check that the median is 5 ## If this results in an error, then the channel identity could be wrong, or ## there may have been an issue with sweep duration during the recording ## process if(!median(round(diff(stim_change_channel[[5]][,1]),0)) == 5) { warning(&quot;File &quot;, i,&quot;: stim change channel identity uncertain&quot;) } ## Determine when the onset of motion occurred according to matlab first_moving_mat &lt;- stim_change_channel[[5]][,1][1] ## Find the first &quot;moving&quot; phase in the log file first_moving_csv &lt;- csv_data_sets[[i]] %&gt;% filter(Trial == &quot;moving&quot;) %&gt;% select(Time) %&gt;% slice(1) %&gt;% as.numeric() ## Find the first &quot;blank&quot; phase in the log file first_blank &lt;- csv_data_sets[[i]] %&gt;% filter(Trial == &quot;blank&quot;) %&gt;% select(Time) %&gt;% slice(1) %&gt;% as.numeric() ## Compute the difference between these two first_mvbl_diff &lt;- first_moving_csv - first_blank ## Check to see if the final row of the metadata is &quot;moving&quot; and truncate ## if not ## This can effectively be done by truncating after the final &quot;moving&quot; phase max_moving_sweep &lt;- max(which(csv_data_sets[[i]]$Trial == &quot;moving&quot;)) first_csv_tmp &lt;- bind_rows(initial, csv_data_sets[[i]]) %&gt;% ## Add 1 to max moving sweep since we tacked on &quot;initial&quot; in previous step ## Then slice to restrict any extraneous partial sweeps slice_head(n = (max_moving_sweep + 1)) %&gt;% ## Add the first event time to &quot;Time&quot; and subtract first_mvbl_diff (~2 secs) ## What this does is shift the log csv&#39;s time stamping to match the matlab ## file&#39;s stim change channel&#39;s time stamping mutate(Time = Time + first_moving_mat - first_mvbl_diff - first_blank) %&gt;% ## Make character version of Time for joining later ## This will be crucial for _join functions mutate(Time_char = as.character(round(Time,3))) ## Duplicate the initialization for ease of setting T0 inception &lt;- initial %&gt;% mutate(Time_char = as.character(round(Time,3))) inception$Trial[1] &lt;- &quot;inception&quot; ## Bind the initialization rows first_csv &lt;- bind_rows(inception, first_csv_tmp) ## Compute stimulus end times first_csv$Stim_end &lt;- c(first_csv$Time[-1], max(first_csv$Time) + 3) ## Get final time final_time &lt;- first_csv$Stim_end[nrow(first_csv)] ## Extract photodiode data ## First generate a time sequence to match to the photodiode trace Time_vec &lt;- seq( from = 0.0000, by = 1 / 25000, length.out = length(photod_default_channel[9][[1]][, 1]) ) ## The key thing is to get a character version of time from this Time_char_vec &lt;- as.character(round(Time_vec, 3)) ## Grab the photodiode data photod_full &lt;- tibble(Photod = photod_default_channel[9][[1]][, 1]) ## Add numeric time photod_full$Time &lt;- seq( from = 0.0000, by = 1 / 25000, length.out = nrow(photod_full) ) options(scipen = 999) photod_full &lt;- photod_full %&gt;% ## Add the character time add_column(Time_char = Time_char_vec) %&gt;% ## Use the charcter time to define a group group_by(Time_char) %&gt;% ## Then average the photodiode within summarise(Photod = mean(Photod)) %&gt;% ungroup() %&gt;% mutate(Time = round(as.numeric(Time_char), 3)) %&gt;% arrange(Time) %&gt;% filter(Time &lt;= final_time) ## Extract all spike data all_spike_dat &lt;- tibble( Time = spikes_default_channel[times_location][[1]][, 1], code = spikes_default_channel[codes_location][[1]][, 1]) %&gt;% ## Characterize time, for purposes of joining later mutate(Time_char = as.character(round(Time, 3))) ## How many distinct neurons are there? cell_ids &lt;- sort(unique(all_spike_dat$code)) n_cells &lt;- 1:length(cell_ids) if(length(n_cells) &gt; 1) { ## if there&#39;s more than one distinct neuron all_spike_dat_tmp &lt;- all_spike_dat %&gt;% ## Group by identity of spiking neuron group_by(code) %&gt;% ## Split into separate dfs, one per neuron group_split() ## Additional cells are labeled as &quot;Spike_n&quot; all_cells &lt;- NULL for (j in n_cells) { #print(j) new_name = paste0(&quot;Spikes_&quot;, cell_ids[j]) all_cells[[j]] &lt;- all_spike_dat_tmp[[j]] ## Consolidate to 3 decimal places all_cells[[j]] &lt;- all_cells[[j]] %&gt;% group_by(Time_char) %&gt;% summarise(code = mean(code)) %&gt;% mutate(code = ceiling(code)) %&gt;% ungroup() %&gt;% mutate(Time = round(as.numeric(Time_char), 3)) %&gt;% arrange(Time) %&gt;% filter(Time &lt;= final_time) names(all_cells[[j]])[match(&quot;code&quot;, names(all_cells[[j]]))] &lt;- new_name ## Replace &quot;j&quot; with 1 to indicate presence/absence of spike rather than ## cell identity all_cells[[j]][new_name] &lt;- 1 ## If the identity is 1, replace &quot;Spikes_1&quot; with just &quot;Spikes&quot; if (new_name == &quot;Spikes_1&quot;) { names(all_cells[[j]])[match(new_name, names(all_cells[[j]]))] &lt;- &quot;Spikes&quot; } } ## Consolidate all_spike_dat &lt;- all_cells %&gt;% ## Tack on additional spike columns reduce(full_join, by = &quot;Time_char&quot;) %&gt;% arrange(Time_char) %&gt;% ## Remove time.n columns but ## Do not remove Time_char select(-contains(&quot;Time.&quot;)) %&gt;% ## Regenerate numeric time mutate( Time = as.numeric(Time_char) ) %&gt;% select(Time, Time_char, Spikes, everything()) %&gt;% filter(Time &lt;= final_time) } else { ## If there&#39;s only 1 neuron all_spike_dat &lt;- all_spike_dat %&gt;% group_by(Time_char) %&gt;% summarise(code = mean(code)) %&gt;% mutate(code = ceiling(code)) %&gt;% ungroup() %&gt;% rename(Spikes = code) %&gt;% mutate(Time = round(as.numeric(Time_char), 3)) %&gt;% arrange(Time) %&gt;% filter(Time &lt;= final_time) %&gt;% select(Time, Time_char, everything()) } options(scipen = 999) mat_data_sets[[i]] &lt;- ## Generate a time sequence from 0 to final_time tibble( Time = seq(from = 0, to = final_time, by = 0.001) ) %&gt;% ## Character-ize it mutate(Time_char = as.character(round(Time, 5))) %&gt;% ## Join in the photodiode data left_join(photod_full, by = &quot;Time_char&quot;) %&gt;% select(-Time.y) %&gt;% rename(Time = Time.x) %&gt;% arrange(Time) %&gt;% ## Join in the spike data left_join(all_spike_dat, by = &quot;Time_char&quot;) %&gt;% select(-Time.y) %&gt;% rename(Time = Time.x) %&gt;% arrange(Time) %&gt;% filter(Time &lt;= final_time) %&gt;% ## Replace NAs with 0 in the Spike columns only mutate( across(starts_with(&quot;Spikes&quot;), ~replace_na(.x, 0)) ) ## Merge the matlab data with the metadata joined_one_full &lt;- mat_data_sets[[i]] %&gt;% ## Join by the character version of time NOT the numerical!! full_join(first_csv, by = &quot;Time_char&quot;) %&gt;% ## Rename columns for clarity of reference rename(Time_mat = Time.x, Time_csv = Time.y) %&gt;% ## Convert character time to numeric time mutate(Time = round(as.numeric(Time_char), 3)) %&gt;% ## Carry metadata forward mutate( Trial = zoo::na.locf(Trial, type = &quot;locf&quot;), Spatial_Frequency = zoo::na.locf(Spatial_Frequency, type = &quot;locf&quot;), Cycles_Per_Pixel = zoo::na.locf(Cycles_Per_Pixel, type = &quot;locf&quot;), Temporal_Frequency = zoo::na.locf(Temporal_Frequency, type = &quot;locf&quot;), Direction = zoo::na.locf(Direction, type = &quot;locf&quot;), Time_csv = zoo::na.locf(Time_csv, type = &quot;locf&quot;), Stim_end = zoo::na.locf(Stim_end, type = &quot;locf&quot;) ) %&gt;% ## Calculate velocity mutate( Speed = round(Temporal_Frequency/Spatial_Frequency, 0), Log2_Speed = log2(Speed) ) ## Add info to metadata metadata_one_full &lt;- first_csv %&gt;% mutate( Speed = round(Temporal_Frequency/Spatial_Frequency, 0), Log2_Speed = log2(Speed), Stim_end_diff = c(0, diff(Stim_end)) ) ## Some quality control checks ## What are the stim time differences? stimtime_diffs &lt;- round(metadata_one_full$Stim_end_diff)[-c(1:2)] ## How many total reps were recorded? stimtime_reps &lt;- length(stimtime_diffs)/3 ## What do we expect the overall structure to look like? stimtime_expectation &lt;- rep(c(1,1,3), stimtime_reps) ## Does reality match our expectations? if (!all(stimtime_diffs == stimtime_expectation)) { ## If you get this, investigate the file further and determine what went ## wrong print(&quot;stimtime issue; investigate&quot;) } ## Sometimes the final sweep gets carried for an indefinite amount of time ## before the investigator manually shuts things off. The following block ## truncates accordingly mark_for_removal &lt;- which(round(metadata_one_full$Stim_end_diff) %not_in% c(1, 3)) if (any(mark_for_removal == 1 | mark_for_removal == 2)) { mark_for_removal &lt;- mark_for_removal[mark_for_removal &gt; 2] } if (length(mark_for_removal) &gt; 0) { metadata_sets[[i]] &lt;- metadata_one_full %&gt;% filter(Time &lt; metadata_one_full[mark_for_removal[1],]$Time) joined_data_sets[[i]] &lt;- joined_one_full %&gt;% filter(Time_mat &lt; metadata_one_full[mark_for_removal[1],]$Time) } else { metadata_sets[[i]] &lt;- metadata_one_full joined_data_sets[[i]] &lt;- joined_one_full } ## Organize the metadata for export in the R environment meta_splits[[i]] &lt;- metadata_sets[[i]] %&gt;% ## Get rid of the non-sweep info filter(!Trial == &quot;inception&quot;) %&gt;% filter(!Trial == &quot;initialization&quot;) %&gt;% ## Group by trial #group_by(Spatial_Frequency, Temporal_Frequency, Direction) %&gt;% ## Split by trial group group_split(Spatial_Frequency, Temporal_Frequency, Direction) data_splits[[i]] &lt;- joined_data_sets[[i]] %&gt;% ## Get rid of the non-sweep info filter(!Trial == &quot;inception&quot;) %&gt;% filter(!Trial == &quot;initialization&quot;) %&gt;% ## Group by trial group_by(Spatial_Frequency, Temporal_Frequency, Direction) %&gt;% ## Split by trial group group_split() ## Do some cleanup so large objects don&#39;t linger in memory rm( first_csv, inception, initial, mat_import, first_csv_tmp, photod_default_channel, spikes_default_channel, photod_full, all_spike_dat, all_spike_dat_tmp, all_cells, metadata_one_full, joined_one_full, joined_data_sets, csv_data_sets, mat_data_sets ) message(&quot;File &quot;, i, &quot;: &quot;, csv_mat_filejoin[i,&quot;basename&quot;], &quot; imported&quot;) gc() } ## [1] 1 endtime &lt;- Sys.time() endtime - starttime ## Total elapsed time ## Time difference of 2.67673 mins ## Tidy up how R has been using RAM by running garbage collection gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 5596189 298.9 16196008 865.0 25306262 1351.6 ## Vcells 204523381 1560.4 915557392 6985.2 1430558425 10914.3 ## Name each data set according to the basename of the file names(metadata_sets) &lt;- csv_mat_filejoin$basename #base_names names(meta_splits) &lt;- csv_mat_filejoin$basename #base_names names(data_splits) &lt;- csv_mat_filejoin$basename #base_names ## Get organized lists of stimuli that were used ## This will ultimately be used for arranging data by stimuli in a sensible ## order metadata_combos &lt;- NULL for (i in 1:length(metadata_sets)) { metadata_combos[[i]] &lt;- metadata_sets[[i]] %&gt;% ## Get unique stimulus parameters distinct(Spatial_Frequency, Temporal_Frequency, Speed, Direction) %&gt;% arrange(Direction) %&gt;% ## Sort by SF (smallest to largest) arrange(desc(Spatial_Frequency)) %&gt;% mutate( ## Set a &quot;plot_order&quot; which provides a running order of stimuli plot_order = 1:length(meta_splits[[i]]), name = paste(Direction, &quot;Deg,&quot;, Speed, &quot;Deg/s&quot;) ) } names(metadata_combos) &lt;- csv_mat_filejoin$basename #base_names What we now have is the following: metadata_sets: contains a list of tibbles (one per imported file), each of which comprises the stimulus log metadata_combos: contains a list of tibbles (one per imported file), each of which comprises the stimulus log re-written in a preferred plotting order meta_splits: a list of lists. The first level of the list corresponds to the file identities. Within each file’s list, there is an additional set of lists, each of which contains a tibble with the log info for a specific stimulus combination. Essentially the same as metadata_sets but split up on a per-stimulus basis. data_splits: another list of lists, arranged in the same hierarchical order as meta_splits. Each tibble herein contains the spike and photodiode data from the matlab file on a per-stimulus basis. Note that since we are only using one example file, each of these lists should only be 1 element long (with the latter two having additional elements within the first element) 5.2 Organizing replicates (required) and binning (optional) It is common to record several different sweeps of a stimulus to collect (somewhat) independent replicates of neural responses. The primary task of this section will be to use the lists from the previous section to gather &amp; label replicates of the same stimulus. A secondary task is to deal with binning, if desired (highly recommended). Depending on the goals of plotting and/or analyses, it may be wise to work with either unbinned spike data or to bin the data at a convenient and sensible interval. I generally choose to work at the following levels: Unbinned Bin size = 10 ms Bin size = 100 ms Important note: Rather than provide separate code for each of these 3 scenarios, I will provide one example. Bin size must be declared beforehand – please pay attention. ## Set bin size here ## Units are in ms (e.g. 10 = 10ms) bin_size = 10 ## 10 or 100 or 1 (1 = &quot;unbinned&quot;) slice_size = NULL slicemin = NULL slicemax = NULL condition = NULL if (bin_size == 10){ slice_size &lt;- 501 slicemin &lt;- 202 slicemax &lt;- 498 condition &lt;- &quot;_binsize10&quot; } else if (bin_size == 100){ slice_size &lt;- 51 slicemin &lt;- 21 slicemax &lt;- 49 condition &lt;- &quot;_binsize100&quot; } else if (bin_size == 1){ slice_size &lt;- NULL slicemin &lt;- NULL slicemax &lt;- NULL condition &lt;- &quot;_unbinned&quot; } else { stop(&quot;bin_size is non-standard&quot;) } all_replicate_data_reorganized &lt;- vector(mode = &quot;list&quot;, length = length(meta_splits)) name_sets &lt;- vector(mode = &quot;list&quot;, length = length(meta_splits)) gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 5596004 298.9 16196008 865.0 25306262 1351.6 ## Vcells 204513787 1560.4 732445914 5588.2 1430558425 10914.3 starttime &lt;- Sys.time() for (i in 1:length(meta_splits)){ ## i = file number print(i) ## We&#39;ll need to collect data at a per-stimulus level and on a per-replicate ## level within the per-stimulus level ## &quot;j&quot; will be used to designate a unique stimulus ## We&#39;ll first create an empty object in which to collect stimulus-specific ## data replicate_data_reorganized &lt;- NULL ## For each of j unique stimuli... for (j in 1:length(meta_splits[[i]])) { # j = {direction,speed} ## Isolate the j-th data d &lt;- data_splits[[i]][[j]] ## And the j-th log data m &lt;- meta_splits[[i]][[j]] %&gt;% group_by(Trial) %&gt;% ## Label separate replicates mutate(Replicate = row_number()) ## Extract a stimulus label to a name_set that will be used later name_sets[[i]][[j]] &lt;- paste(m$Direction[1], &quot;Deg,&quot;, m$Speed[1], &quot;Deg/s&quot;) ## Set up a temporary object to deal with per-replicate data replicates_ordered &lt;- NULL ## &quot;k&quot; will be used to designate replicate number for (k in 1:max(m$Replicate)){ tmp &lt;- m %&gt;% filter(Replicate == k) ## If you have a complete replicate (i.e., blank, stationary, moving) if (nrow(tmp) == 3 ) { ## Grab the specific data doot &lt;- d %&gt;% filter(Time &gt;= min(tmp$Time)) %&gt;% filter(Time &lt;= max(tmp$Stim_end)) ## Add bin information doot$bin &lt;- rep(1:ceiling(nrow(doot)/bin_size), each = bin_size)[1:nrow(doot)] if (bin_size == 1) { ## IF YOU ARE NOT BINNING, RUN THIS: replicates_ordered[[k]] &lt;- doot %&gt;% mutate( ## Construct a standardized time within the sweep Time_stand = Time_mat - min(Time_mat), ## When does the sweep begin Time_begin = min(Time_mat), ## When does the sweep end Time_end = max(Time_mat), ## Delineate the end of the blank phase Blank_end = tmp$Stim_end[1] - min(Time_mat), ## Delineate the end of the stationary phase Static_end = tmp$Stim_end[2] - min(Time_mat), ## Label the replicate number Replicate = k ) %&gt;% ## Bring stim info to first few columns select(Speed, Spatial_Frequency, Temporal_Frequency, Direction, everything()) %&gt;% ## Just in case there some hang over filter(Time_stand &gt;= 0) } else { ## IF YOU ARE BINNING, RUN THIS: ## First grab time and meta info time_and_meta &lt;- doot %&gt;% ## WITHIN EACH BIN: group_by(bin) %&gt;% summarise( ## Label the trial Trial = first(Trial), ## Midpoint of bin Time_bin_mid = mean(Time_mat), ## Bin beginning Time_bin_begin = min(Time_mat), ## Bin end Time_bin_end = max(Time_mat), ## Spike rate = sum of spikes divided by elapsed time Spike_rate = sum(Spikes) / (max(Time_mat) - min(Time_mat)), Photod_mean = mean(Photod) ) ## Now deal with Spike_N columns hold_spike_n &lt;- doot %&gt;% select(starts_with(&quot;Spikes_&quot;)) %&gt;% add_column(bin = doot$bin) %&gt;% add_column(Time_mat = doot$Time_mat) %&gt;% group_by(bin) %&gt;% summarise(across(starts_with(&quot;Spikes_&quot;), ~ sum(.x) / (max(Time_mat) - min(Time_mat)))) ## Put them together replicates_ordered[[k]] &lt;- time_and_meta %&gt;% left_join(hold_spike_n, by = &quot;bin&quot;) %&gt;% mutate( ## Add in metadata (following same definitions above) Time_stand = Time_bin_mid - min(Time_bin_mid), Blank_end = tmp$Stim_end[1] - min(Time_bin_mid), Static_end = tmp$Stim_end[2] - min(Time_bin_mid), Spatial_Frequency = m$Spatial_Frequency[1], Temporal_Frequency = m$Temporal_Frequency[1], Speed = m$Speed[1], Direction = m$Direction[1], Replicate = k ) %&gt;% ## Bring stim info to first few columns select(Speed, Spatial_Frequency, Temporal_Frequency, Direction, everything()) %&gt;% ## Just in case there some hang over filter(Time_stand &gt;= 0) %&gt;% filter(bin &lt; slice_size + 1) rm(time_and_meta, hold_spike_n) } } } ## Now insert it within the collector of per-stimulus data replicate_data_reorganized[[j]] &lt;- replicates_ordered %&gt;% bind_rows() ## Now insert it within the overall data collector all_replicate_data_reorganized[[i]][[j]] &lt;- replicate_data_reorganized[[j]] ## Toss out temporary objects and clean up rm(replicates_ordered, d, m, tmp) gc() } } ## [1] 1 endtime &lt;- Sys.time() endtime - starttime ## Time difference of 2.527167 mins gc() ## used (Mb) gc trigger (Mb) max used (Mb) ## Ncells 5600940 299.2 16196008 865.0 25306262 1351.6 ## Vcells 208863355 1593.6 585956732 4470.5 1430558425 10914.3 for (i in 1:length(all_replicate_data_reorganized)) { for (j in 1:length(all_replicate_data_reorganized[[i]])) { names(all_replicate_data_reorganized[[i]])[[j]] &lt;- name_sets[[i]][[j]] } } names(all_replicate_data_reorganized) &lt;- csv_mat_filejoin$basename #base_names At the end of this process, here is how all_replicate_data_reorganized[[1]] should look: ## How long is this list? length(all_replicate_data_reorganized[[1]]) ## [1] 80 ## This object contains 48 individual tibbles. Here&#39;s an example of one ## pulled at random all_replicate_data_reorganized[[1]][sample(1:48, size = 1)] ## $`45 Deg, 128 Deg/s` ## # A tibble: 3,507 × 16 ## Speed Spatial_F…¹ Tempo…² Direc…³ bin Trial Time_…⁴ Time_…⁵ Time_…⁶ Spike…⁷ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 128 0.0442 5.66 45 1 blank 99.3 99.3 99.3 0 ## 2 128 0.0442 5.66 45 2 blank 99.4 99.4 99.4 0 ## 3 128 0.0442 5.66 45 3 blank 99.4 99.4 99.4 0 ## 4 128 0.0442 5.66 45 4 blank 99.4 99.4 99.4 111. ## 5 128 0.0442 5.66 45 5 blank 99.4 99.4 99.4 0 ## 6 128 0.0442 5.66 45 6 blank 99.4 99.4 99.4 0 ## 7 128 0.0442 5.66 45 7 blank 99.4 99.4 99.4 0 ## 8 128 0.0442 5.66 45 8 blank 99.4 99.4 99.4 0 ## 9 128 0.0442 5.66 45 9 blank 99.4 99.4 99.4 0 ## 10 128 0.0442 5.66 45 10 blank 99.4 99.4 99.4 0 ## # … with 3,497 more rows, 6 more variables: Photod_mean &lt;dbl&gt;, Spikes_0 &lt;dbl&gt;, ## # Time_stand &lt;dbl&gt;, Blank_end &lt;dbl&gt;, Static_end &lt;dbl&gt;, Replicate &lt;int&gt;, and ## # abbreviated variable names ¹​Spatial_Frequency, ²​Temporal_Frequency, ## # ³​Direction, ⁴​Time_bin_mid, ⁵​Time_bin_begin, ⁶​Time_bin_end, ⁷​Spike_rate ## To consolidate this, you can use bind_rows() all_replicate_data_reorganized[[1]] %&gt;% bind_rows ## # A tibble: 284,067 × 16 ## Speed Spatial_F…¹ Tempo…² Direc…³ bin Trial Time_…⁴ Time_…⁵ Time_…⁶ Spike…⁷ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1024 0.0156 16 0 1 blank 181. 181. 181. 0 ## 2 1024 0.0156 16 0 2 blank 181. 181. 181. 0 ## 3 1024 0.0156 16 0 3 blank 181. 181. 181. 0 ## 4 1024 0.0156 16 0 4 blank 181. 181. 181. 0 ## 5 1024 0.0156 16 0 5 blank 181. 181. 181. 0 ## 6 1024 0.0156 16 0 6 blank 181. 181. 181. 0 ## 7 1024 0.0156 16 0 7 blank 181. 181. 181. 0 ## 8 1024 0.0156 16 0 8 blank 181. 181. 181. 222. ## 9 1024 0.0156 16 0 9 blank 181. 181. 181. 222. ## 10 1024 0.0156 16 0 10 blank 181. 181. 181. 111. ## # … with 284,057 more rows, 6 more variables: Photod_mean &lt;dbl&gt;, ## # Spikes_0 &lt;dbl&gt;, Time_stand &lt;dbl&gt;, Blank_end &lt;dbl&gt;, Static_end &lt;dbl&gt;, ## # Replicate &lt;int&gt;, and abbreviated variable names ¹​Spatial_Frequency, ## # ²​Temporal_Frequency, ³​Direction, ⁴​Time_bin_mid, ⁵​Time_bin_begin, ## # ⁶​Time_bin_end, ⁷​Spike_rate Bear in mind that I am specifically describing all_replicate_data_reorganized[[1]], NOT all_replicate_data_reorganized. This is because all_replicate_data_reorganized will itself be a list that is as long as the number of distinct data files you are feeding into all of the above. Since there is only 1 example file, all_replicate_data_reorganized is a list that is only 1 entry long at its top level, and within that list is set of 48 lists (one per distinct stimulus), each of which contains a stim-specific tibble of data. 5.3 Data export It is highly recommended that you export all_replicate_data_reorganized. I generally export all_replicate_data_reorganized as a separate csv file for each of the following conditions: Unbinned Bin size = 10 ms Bin size = 100 ms This section will provide code to export each of the above, assuming you used those bin sizes in the previous section. Please be sure to change file naming conventions and other parameters in the event you chose a different bin_size in the previous section. ## Declare export destination export_path &lt;- &quot;./data/&quot; ## The &quot;condition&quot; will be appended to the file name. ## Export each tibble within all_replicate_data_reorganized for (i in 1:length(all_replicate_data_reorganized)) { print(i) dat &lt;- all_replicate_data_reorganized[[i]] %&gt;% bind_rows() write_csv( dat, file = paste0( export_path, names(all_replicate_data_reorganized)[i], condition, &quot;.csv&quot; ) ) rm(dat) } Re-run the above chunk of code per condition you seek to export. For me, this process creates 3 files: 2023-02-16_001_unbinned.csv, 2023-02-16_001_binsize10.csv, and 2023-02-16_001_binsize100.csv.  "],["raster-and-mean-spike-rate-plots.html", "6 Raster and mean spike rate plots 6.1 Data sets 6.2 Raster plot 6.3 Mean spike rate plots 6.4 Export to PDF", " 6 Raster and mean spike rate plots 6.1 Data sets This section will rely on some of the unbinned and binned data we exported in final steps of the previous section. We’ll start by loading in information ## File paths and basenames of _unbinned.csv files unbinned_filelist &lt;- list.files(&quot;./data/&quot;, pattern = &quot;_unbinned.csv&quot;, full.names = TRUE) unbinned_basenames &lt;- unbinned_filelist %&gt;% str_remove(&quot;./data/&quot;) %&gt;% str_remove(&quot;_unbinned.csv&quot;) ## File paths and basenames of _binsize10.csv files bin10_filelist &lt;- list.files(&quot;./data/&quot;, pattern = &quot;_binsize10.csv&quot;, full.names = TRUE) bin10_basenames &lt;- bin10_filelist %&gt;% str_remove(&quot;./data/&quot;) %&gt;% str_remove(&quot;_binsize10.csv&quot;) ## File paths and basenames of _binsize100.csv files bin100_filelist &lt;- list.files(&quot;./data/&quot;, pattern = &quot;_binsize100.csv&quot;, full.names = TRUE) bin100_basenames &lt;- bin100_filelist %&gt;% str_remove(&quot;./data/&quot;) %&gt;% str_remove(&quot;_binsize100.csv&quot;) You should have something similar to the following: unbinned_filelist; unbinned_basenames; bin10_filelist; bin10_basenames; bin100_filelist; bin100_basenames ## [1] &quot;./data/2023-02-16_001_unbinned.csv&quot; ## [1] &quot;2023-02-16_001&quot; ## [1] &quot;./data/2023-02-16_001_binsize10.csv&quot; ## [1] &quot;2023-02-16_001&quot; ## [1] &quot;./data/2023-02-16_001_binsize100.csv&quot; ## [1] &quot;2023-02-16_001&quot; 6.2 Raster plot Here is an example of a raster plot, using the wrangled data generated in the previous section. This type of plot shows the timing of spike events within each replicate sweep, and for our purposes, we’ll produce a view of this for each of the various stimulus conditions It is important to note that we will need unbinned data for this. This is because a raster plot shows discrete spiking events through the course of a time sweep. Here, we will use ggplot to create a plot with: Data sub-plotted by stimulus (i.e., Speed and Direction) Standardized sweep time on the x-axis, with delineation among blank, stationary, and moving phases using red, yellow, and green undershading. The y-axis will indicate replicate number A black tick each time a spike is observed. Absence of black tick = no observed spike ## For each unbinned file, generate a raster plot rasterplots &lt;- NULL for (i in 1:length(unbinned_filelist)) { ## Read in the data unbinned_data &lt;- read_csv(unbinned_filelist[i]) %&gt;% as_tibble() ## determine the max number of replicates max_reps &lt;- max(unbinned_data$Replicate) ## get unique speeds sorted_speeds &lt;- unbinned_data$Speed %&gt;% unique %&gt;% sort(decreasing = TRUE) ## Generate the code for the ggplot and save it as rasterplots[[i]] rasterplots[[i]] &lt;- unbinned_data %&gt;% ## Remove any rows where spiking does not occur in the Spikes column filter(Spikes == 1) %&gt;% ## Convert Trial and Speed into factors and specify their level ordering ## This will make it easier to get the subplots in the order we want them mutate( Trial = factor(Trial, levels = c(&quot;blank&quot;, &quot;stationary&quot;, &quot;moving&quot;)), Speed = factor(Speed, levels = sorted_speeds)) %&gt;% ggplot(aes(x = Time_stand, y = Replicate)) + ## The next three blocks will undershade each subplot according to stimulus ## phase (i.e., blank, stationary, moving) annotate(&quot;rect&quot;, xmin = 0, xmax = first(unbinned_data$Blank_end), ymin = 0.5, ymax = max_reps + 0.5, alpha = 0.075, color = NA, fill = &quot;red&quot;) + annotate(&quot;rect&quot;, xmin = first(unbinned_data$Blank_end), xmax = first(unbinned_data$Static_end), ymin = 0.5, ymax = max_reps + 0.5, alpha = 0.075, color = NA, fill = &quot;darkgoldenrod1&quot;) + annotate(&quot;rect&quot;, xmin = first(unbinned_data$Static_end), xmax = 5, ymin = 0.5, ymax = max_reps + 0.5, alpha = 0.075, color = NA, fill = &quot;forestgreen&quot;) + ## Up to 10 replicates were used, so we will force the y-axis to go to 10 scale_y_continuous( limits = c(0.5, max_reps + 0.5), expand = c(0, 0), breaks = c(max_reps/2, max_reps) ) + ## There are multiple ways to plot a spike event. Since 100% of the rows in ## this filtered data set are spike events, we can simply plot a symbol at ## each time (Time_stand) that appears in the data. The `|` symbol is a good ## choice. geom_point(pch = &#39;|&#39;, size = 1.5) + xlab(&quot;Time (sec)&quot;) + ggtitle(paste0(unbinned_basenames[i], &quot; raster&quot;)) + ## Use facet_grid() to create a grid of subplots. Rows will correspond to ## Speeds, and columns correspond to Directions facet_grid(rows = vars(Speed), cols = vars(Direction)) + theme_classic() + theme(legend.position = &#39;none&#39;, panel.spacing = unit(0.1, &quot;lines&quot;)) ## Clean up rm(unbinned_data) } rasterplots is now an object in the environment that contains one plot per imported data file. To plot, simply call the index of the file you wish to see. Since we only have 1 example file, we’ll showcase the only plot here: ## Here&#39;s the raster plot rasterplots[[1]] 6.3 Mean spike rate plots To visualize the mean spike rate over the course of the stimulus sweep, I typically elect to use 100ms-binned data. This format of the data allows me to see the salient spike rate patterns without focusing on every little variation in the data. Here, we will use ggplot to create a plot with: Data sub-plotted by stimulus (i.e., Speed and Direction) Standardized sweep time on the x-axis, with delineation among blank, stationary, and moving phases A black line to indicate the mean spike rate, along with a grey ribbon to show +/- 1 S.E.M. ## For each 100-ms binned file, generate a mean spike plot bin100_msr_plots &lt;- NULL for (i in 1:length(bin100_filelist)) { ## Read in the data bin100_data &lt;- read_csv(bin100_filelist[i]) %&gt;% as_tibble() ## get unique speeds sorted_speeds &lt;- bin100_data$Speed %&gt;% unique %&gt;% sort(decreasing = TRUE) ## Compute SE and other metrics and add this to our data set dataslices_100 &lt;- bin100_data %&gt;% mutate( Speed = factor(Speed, levels = sorted_speeds)) %&gt;% ## Split by direction and speed, because we will use those to define each ## subplot group_split(Direction, Speed) %&gt;% ## Group by time bin purrr::map(group_by, bin) %&gt;% ## Within each time bin, compute the following: purrr::map(transmute, ## first() can be used for metadata such as Speed or Direction Speed = first(Speed), Direction = first(Direction), ## I generally compute the mean within each bin for the following: Time_stand = mean(Time_stand), Blank_end = mean(Blank_end), Static_end = mean(Static_end), Mean_spike_rate = mean(Spike_rate), ## To get SE, divide s.d. by the square root of sample size Spike_rate_SE = sd(Spike_rate)/sqrt(n()), Mean_photod_rate = mean(Photod_mean), ## SE of photodiode Photod_SE = sd(Photod_mean)/sqrt(n()) ) %&gt;% purrr::map(ungroup) %&gt;% bind_rows() ## Generate the mean spike rate plot using ggplot bin100_msr_plots[[i]] &lt;- dataslices_100 %&gt;% ## The same code block can be used to generate either the mean spike rate ## (shown below) or photodiode trace (commented out) ggplot(aes(x = Time_stand, y = Mean_spike_rate #Mean_photod_rate )) + ## We&#39;ll actually start by placing red, yellow, and green vertical lines to ## distinguish between blank, stationary, and moving phases ## This comes first so that it is the bottom-most layer and doesn&#39;t obstruct ## the data geom_vline(xintercept = 0, col = &quot;red&quot;) + geom_vline(xintercept = first(dataslices_100$Blank_end), col = &quot;darkgoldenrod1&quot;) + geom_vline(xintercept = first(dataslices_100$Static_end), col = &quot;forestgreen&quot;) + ## We&#39;ll use `geom_ribbon()` to shade in the SE traces geom_ribbon(aes( ymin = Mean_spike_rate - Spike_rate_SE, ymax = Mean_spike_rate + Spike_rate_SE # ymin = Mean_photod_rate - Photod_SE, # ymax = Mean_photod_rate + Photod_SE ), fill = &quot;grey80&quot;) + ## `geom_line()` will be used to draw the mean spike rate itself on top of ## the SE traces geom_line(linewidth = 0.05) + ## Add a title to help us know what cell this is ggtitle(bin100_basenames[i], &quot; mean spike rate (100-ms bins)&quot;) + xlab(&quot;Time (sec)&quot;) + ylab(&quot;Spike rate (spikes/sec)&quot;) + ## To sub-plot by Speed and Direction, I typically use `facet_grid()`. This ## method allows me to explicitly declare what the row- and column-wise ## grouping variables are facet_grid(rows = vars(Speed), cols = vars(Direction)) + theme_classic() rm(bin100_data, dataslices_100) } bin100_msr_plots is now an object in the environment that contains one plot per imported data file. To plot, simply call the index of the file you wish to see. Since we only have 1 example file, we’ll showcase the only plot here: ## Here&#39;s the spike rate plot bin100_msr_plots[[1]] 6.4 Export to PDF Should you elect to export either of these sets of plots as PDFs, here is an example of what you could do. ## Use the `pdf()` function to start the graphics device driver for producing ## PDFs ## Aspects such as page size and centering mode can be adjusted for (i in 1:length(rasterplots)) { pdf(file = paste0(&quot;./plot_pdfs/&quot;, unbinned_basenames[i], &quot;_raster.pdf&quot;), width = 22, height = 12, pagecentre = TRUE, colormodel = &quot;srgb&quot;) ## Now add the plot to the PDF simply by calling plot() plot(rasterplots[[i]]) ## To declare an end to this PDF writing session, use `dev.off()` dev.off() } for (i in 1:length(bin100_msr_plots)) { pdf(file = paste0(&quot;./plot_pdfs/&quot;, bin100_basenames[i], &quot;_raster.pdf&quot;), width = 22, height = 12, pagecentre = TRUE, colormodel = &quot;srgb&quot;) plot(bin100_msr_plots[[i]]) dev.off() } "],["direction-tuning.html", "7 Direction tuning 7.1 Data import and baseline rate measurement 7.2 Using the full 3-sec motion epoch 7.3 Using 40-200ms (“initial transient”) 7.4 Using 1500-3000ms (“steady state”) 7.5 Using 0-500 ms (arbitrary epoch length) 7.6 Construct a multi-panel plot", " 7 Direction tuning This type of plot is so fundamental to the examination of LM and nBOR, it deserves a chapter unto itself. The majority of the information going into polar tuning plots is the same as those in previous chapters. Unlike raster and mean spike rate plots, however, direction tuning plots require deliberate choices from the investigator to delineate a “baseline” spike rate to be distinguished from the spike rate during the stimulus of interest (in our case, global motion patterns). The conditions that define the baseline and motion epochs require explicit definitions form the investigator. Seeing the mean spike rate plot from the previous chapter will help give context to some of these decisions [INSERT IMAGE HERE]. Importantly, at the onset of the blank and the stationary phases, it is common to observe an initial transient response – a sharp increase in the spike rate of the neuron – followed by a return to a steady state. For our purposes, the baseline will be defined as the steady state response during the stationary phase of the stimulus. We will collect this steady state response rate across all stimulus conditions (i.e., varying combos of speed and direction), and will simply average all of those response rates to attain our baseline rate (and its SEM). Please note that it is up to the investigator to determine if this definition is appropriate, especially if other stimulus presentations are being used. For the “motion epoch”, I will use a few different definitions. Some of these conventions are informed by previous work (e.g., Smyth et al. 2022), whereas others are just based on a rough approximation of what may be appropriate for these data. The definitions of the motion epochs will be: The entire 3-sec motion epoch The “initial transient” phase: 40-200ms after the onset of motion (as used in Smyth et al. 2022) The “steady state” phase: 1500-3000ms after the onset of motion, a.k.a., the second half of the motion stimulus (as used in Smyth et al. 2022) The first 500 ms of the motion phase. This is an arbitrary definition for demonstrative purposes only, but somewhat informed by observing the patterns in the mean spike rate plot As we construct polar plots, two additional aspects will be included in the plots: The “preferred direction” of the neuron, as defined by the vector sum method [insert citation]. This will be shown as a single grey line in the plot that indicates the preferred direction. Bear in mind that this value may not always be meaningful, particularly in cases of multi-modal responses. The Sensitivity Index (SI) of the neuron, as defined by citation. This metric calculates the narrowness of tuning. An SI of 1 indicates strong response to a single direction whereas 0 indicates similar response across all investigated directions. 7.1 Data import and baseline rate measurement We will use the 10-ms binned data for these examples. We’ll read in the file and then extract the baseline rate as defined above. The code will be written such that it can batch-process more than one file if desired. bin10_data &lt;- NULL polar_directions &lt;- NULL baseline_summary_df &lt;- NULL for (i in 1:length(bin10_filelist)) { ## Read in the data bin10_data[[i]] &lt;- read_csv(bin10_filelist[i], show_col_types = FALSE) %&gt;% as_tibble() ## get unique speeds sorted_speeds &lt;- bin10_data[[i]]$Speed %&gt;% unique %&gt;% sort(decreasing = TRUE) ## Again, we will set Speed as an ordered factor to help control plotting ## later on bin10_data[[i]] &lt;- bin10_data[[i]] %&gt;% mutate( Speed = factor(Speed, levels = sorted_speeds)) ## Determine unique directions polar_directions[[i]] &lt;- bin10_data[[i]]$Direction %&gt;% unique() ## Extract all rows corresponding to our desired baseline epoch baseline_df &lt;- bin10_data[[i]] %&gt;% filter(Time_stand &gt;= Blank_end + 0.5) %&gt;% ## 0.5 sec after blank end filter(Time_stand &lt;= Static_end - 0.05) ## 0.05 sec before static end ## Compute the mean baseline global_base &lt;- mean(baseline_df$Spike_rate) ## Compute the SE global_base_se &lt;- sd(baseline_df$Spike_rate) / sqrt(length(baseline_df$Spike_rate) ) ## Construct a summary data frame baseline_summary_df[[i]] &lt;- baseline_df %&gt;% group_by(Speed) %&gt;% summarize( speed_specific_baseline = mean(Spike_rate), speed_specific_baseline_se = sd(Spike_rate)/sqrt(length(Spike_rate)), global_baseline = global_base, global_baseline_se = global_base_se ) ## This tibble contains speed-specific baselines (and SE) along with the ## global mean baseline (and SE) baseline_summary_df[[i]] } 7.2 Using the full 3-sec motion epoch This will be referred to as the “naive” approach in the code below ## global baseline values, full 3-sec motion period bin10_polar_naive &lt;- NULL for (i in 1:length(bin10_filelist)) { naive_df &lt;- bin10_data[[i]] %&gt;% filter(Time_stand &gt; Static_end) %&gt;% group_by(Speed, Direction, Replicate) %&gt;% summarize( mean_spike_rate = mean(Spike_rate) ) naive_360 &lt;- naive_df %&gt;% filter(Direction == 0) %&gt;% transmute( Speed = Speed, Direction = 360, Replicate = Replicate, mean_spike_rate = mean_spike_rate) naive_vecsum_df &lt;- naive_df %&gt;% ungroup() %&gt;% drop_na(mean_spike_rate) %&gt;% group_split(Speed) %&gt;% map(group_by, Direction) %&gt;% ## NOTE: FOLLOWING THE JNP AND CB PAPERS, WE ARE SUBTRACTING BASELINE HERE AND ## THEN IF ANY AVERAGED FIRING RATES ARE NEGATIVE, THEY ARE SHIFTED SO THE ## LOWEST ONE IS ZERO. THE GENERATED PLOTS STILL SHOW NON-BASELINE-SUBTRACTED ## FIRING RATES (AND BASELINE AS A RED RING), BUT COMPUTATION OF VECTOR SUM ## AND SI HAVE BEEN BASELINE SUBTRACTED (AND THE ENTIRE CURVE IS SHIFTED ## UPWARDS IF ANY PART IS NEGATIVE) map( summarize, mean_spike_rate = mean(mean_spike_rate) - unique(baseline_summary_df[[i]]$global_baseline), Speed = first(Speed) ) %&gt;% map(mutate, mean_spike_rate = case_when( min(mean_spike_rate) &lt; 0 ~ mean_spike_rate + abs(min(mean_spike_rate)), TRUE ~ mean_spike_rate )) %&gt;% map( transmute, x = cos(Direction * pi / 180) * mean_spike_rate, y = sin(Direction * pi / 180) * mean_spike_rate, Speed = first(Speed) ) %&gt;% map(summarise, x = mean(x), y = mean(y), Speed = first(Speed)) %&gt;% map(transmute, vector_sum = (atan2(y, x) * 180 / pi) %% 360, Speed = first(Speed)) %&gt;% bind_rows() naive_si_df &lt;- naive_df %&gt;% ungroup() %&gt;% drop_na(mean_spike_rate) %&gt;% group_split(Speed) %&gt;% map(group_by, Direction) %&gt;% map( summarize, mean_spike_rate = mean(mean_spike_rate) - unique(baseline_summary_df[[i]]$global_baseline), Speed = first(Speed) ) %&gt;% map(mutate, mean_spike_rate = case_when( min(mean_spike_rate) &lt; 0 ~ mean_spike_rate + abs(min(mean_spike_rate)), TRUE ~ mean_spike_rate )) %&gt;% map( transmute, a = (sin(Direction * pi / 180) * mean_spike_rate), b = (cos(Direction * pi / 180) * mean_spike_rate), c = mean(mean_spike_rate), Speed = first(Speed) ) %&gt;% map( summarise, a = mean(a), b = mean(b), c = mean(c), Speed = first(Speed) ) %&gt;% map(transmute, si = sqrt(a ^ 2 + b ^ 2) / c, Speed = first(Speed)) %&gt;% bind_rows() naive_data &lt;- naive_df %&gt;% bind_rows(naive_360) %&gt;% left_join(naive_vecsum_df, by = &quot;Speed&quot;) %&gt;% left_join(naive_si_df, by = &quot;Speed&quot;) %&gt;% drop_na(mean_spike_rate) naive_max_y &lt;- max(naive_data$mean_spike_rate) bin10_polar_naive[[i]] &lt;- naive_data %&gt;% left_join(baseline_summary_df[[i]], by = &quot;Speed&quot;) %&gt;% ggplot(aes(x = Direction, y = mean_spike_rate)) + geom_ribbon(aes( x = Direction, ymin = global_baseline - global_baseline_se, ymax = global_baseline + global_baseline_se ), fill = &quot;darkgoldenrod1&quot;) + stat_smooth(method = &quot;glm&quot;, formula = y ~ ns(x, length(polar_directions[[i]])), linewidth = 0.3, color = &quot;forestgreen&quot;) + geom_point(color = &quot;#333132&quot;) + geom_label(aes(label = round(si, 2) , x = 315, y = naive_max_y * 1.2), size = 3) + geom_vline(aes(xintercept = vector_sum), colour = &quot;grey30&quot;, size = 0.75) + coord_polar(direction = 1, start = pi / 2) + scale_x_continuous( breaks = c(0, 90, 180, 270), expand = c(0, 0), limits = c(0, 360) ) + scale_y_continuous( #trans = &quot;sqrt&quot;, limits = c(0, naive_max_y * 1.2) ) + facet_grid(cols = vars(Speed)) + ggtitle(&quot;Full 3-sec motion&quot;) + ylab(&quot;Spike rate (spikes/sec)&quot;) + theme_minimal() } ## `summarise()` has grouped output by &#39;Speed&#39;, &#39;Direction&#39;. You can override ## using the `.groups` argument. ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. bin10_polar_naive[[1]] 7.3 Using 40-200ms (“initial transient”) ## global baseline values, 40-200 msec motion period bin10_polar_cbit &lt;- NULL for (i in 1:length(bin10_filelist)) { cbit_df &lt;- bin10_data[[i]] %&gt;% filter(Time_stand &gt;= Static_end + 0.04) %&gt;% filter(Time_stand &lt;= Static_end + 0.2) %&gt;% group_by(Speed, Direction, Replicate) %&gt;% summarize( mean_spike_rate = mean(Spike_rate) ) cbit_360 &lt;- cbit_df %&gt;% filter(Direction == 0) %&gt;% transmute( Speed = Speed, Direction = 360, Replicate = Replicate, mean_spike_rate = mean_spike_rate) cbit_vecsum_df &lt;- cbit_df %&gt;% ungroup() %&gt;% drop_na(mean_spike_rate) %&gt;% group_split(Speed) %&gt;% map(group_by, Direction) %&gt;% map(summarize, mean_spike_rate = mean(mean_spike_rate) - unique(baseline_summary_df[[i]]$global_baseline), Speed = first(Speed)) %&gt;% map(mutate, mean_spike_rate = case_when(min(mean_spike_rate) &lt; 0 ~ mean_spike_rate + abs(min(mean_spike_rate)), TRUE ~ mean_spike_rate)) %&gt;% map(transmute, x = cos(Direction * pi / 180) * mean_spike_rate, y = sin(Direction * pi / 180) * mean_spike_rate, Speed = first(Speed) ) %&gt;% map(summarise, x = mean(x), y = mean(y), Speed = first(Speed)) %&gt;% map(transmute, vector_sum = (atan2(y, x) * 180 / pi) %% 360, Speed = first(Speed) ) %&gt;% bind_rows() cbit_si_df &lt;- cbit_df %&gt;% ungroup() %&gt;% drop_na(mean_spike_rate) %&gt;% group_split(Speed) %&gt;% map(group_by, Direction) %&gt;% map(summarize, mean_spike_rate = mean(mean_spike_rate) - unique(baseline_summary_df[[i]]$global_baseline), Speed = first(Speed)) %&gt;% map(mutate, mean_spike_rate = case_when(min(mean_spike_rate) &lt; 0 ~ mean_spike_rate + abs(min(mean_spike_rate)), TRUE ~ mean_spike_rate)) %&gt;% map(transmute, a = (sin(Direction * pi / 180) * mean_spike_rate), b = (cos(Direction * pi / 180) * mean_spike_rate), c = mean(mean_spike_rate), Speed = first(Speed) ) %&gt;% map(summarise, a = mean(a), b = mean(b), c = mean(c), Speed = first(Speed)) %&gt;% map(transmute, si = sqrt(a ^ 2 + b ^ 2) / c, Speed = first(Speed) ) %&gt;% bind_rows() cbit_data &lt;- cbit_df %&gt;% bind_rows(cbit_360) %&gt;% left_join(cbit_vecsum_df, by = &quot;Speed&quot;) %&gt;% left_join(cbit_si_df, by = &quot;Speed&quot;) %&gt;% drop_na(mean_spike_rate) cbit_max_y &lt;- max(cbit_data$mean_spike_rate) bin10_polar_cbit[[i]] &lt;- cbit_data %&gt;% left_join(baseline_summary_df[[i]], by = &quot;Speed&quot;) %&gt;% ggplot(aes(x = Direction, y = mean_spike_rate)) + geom_ribbon(aes( x = Direction, ymin = global_baseline - global_baseline_se, ymax = global_baseline + global_baseline_se ), fill = &quot;darkgoldenrod1&quot;) + stat_smooth(method = &quot;glm&quot;, formula = y ~ ns(x, length(polar_directions[[i]])), linewidth = 0.3, color = &quot;forestgreen&quot;) + geom_point(color = &quot;#333132&quot;) + geom_label(aes(label = round(si, 2) , x = 315, y = cbit_max_y * 1.2), size = 3) + geom_vline(aes(xintercept = vector_sum), colour = &quot;grey30&quot;, size = 0.75) + coord_polar(direction = 1, start = pi/2) + scale_x_continuous( breaks = c(0, 90, 180, 270), expand = c(0, 0), limits = c(0, 360) ) + scale_y_continuous( #trans = &quot;sqrt&quot;, limits = c(0, cbit_max_y * 1.2) ) + facet_grid(cols = vars(Speed)) + ggtitle(&quot;40-200 ms (initial transient) motion&quot;) + ylab(&quot;Spike rate (spikes/sec)&quot;) + theme_minimal() } ## `summarise()` has grouped output by &#39;Speed&#39;, &#39;Direction&#39;. You can override ## using the `.groups` argument. bin10_polar_cbit[[1]] ## Warning: Removed 149 rows containing missing values (`geom_smooth()`). 7.4 Using 1500-3000ms (“steady state”) ## global baseline values, second half of motion period bin10_polar_cbss &lt;- NULL for (i in 1:length(bin10_filelist)) { cbss_df &lt;- bin10_data[[i]] %&gt;% filter(Time_stand &gt; Static_end + 1.5) %&gt;% filter(Time_stand &lt; 4.95) %&gt;% group_by(Speed, Direction, Replicate) %&gt;% summarize( mean_spike_rate = mean(Spike_rate) ) cbss_360 &lt;- cbss_df %&gt;% filter(Direction == 0) %&gt;% transmute( Speed = Speed, Direction = 360, Replicate = Replicate, mean_spike_rate = mean_spike_rate) cbss_vecsum_df &lt;- cbss_df %&gt;% ungroup() %&gt;% drop_na(mean_spike_rate) %&gt;% group_split(Speed) %&gt;% map(group_by, Direction) %&gt;% map(summarize, mean_spike_rate = mean(mean_spike_rate) - unique(baseline_summary_df[[i]]$global_baseline), Speed = first(Speed)) %&gt;% map(mutate, mean_spike_rate = case_when(min(mean_spike_rate) &lt; 0 ~ mean_spike_rate + abs(min(mean_spike_rate)), TRUE ~ mean_spike_rate)) %&gt;% map(transmute, x = cos(Direction * pi / 180) * mean_spike_rate, y = sin(Direction * pi / 180) * mean_spike_rate, Speed = first(Speed) ) %&gt;% map(summarise, x = mean(x), y = mean(y), Speed = first(Speed)) %&gt;% map(transmute, vector_sum = (atan2(y, x) * 180 / pi) %% 360, Speed = first(Speed) ) %&gt;% bind_rows() cbss_si_df &lt;- cbss_df %&gt;% ungroup() %&gt;% drop_na(mean_spike_rate) %&gt;% group_split(Speed) %&gt;% map(group_by, Direction) %&gt;% map(summarize, mean_spike_rate = mean(mean_spike_rate) - unique(baseline_summary_df[[i]]$global_baseline), Speed = first(Speed)) %&gt;% map(mutate, mean_spike_rate = case_when(min(mean_spike_rate) &lt; 0 ~ mean_spike_rate + abs(min(mean_spike_rate)), TRUE ~ mean_spike_rate)) %&gt;% map(transmute, a = (sin(Direction * pi / 180) * mean_spike_rate), b = (cos(Direction * pi / 180) * mean_spike_rate), c = mean(mean_spike_rate), Speed = first(Speed) ) %&gt;% map(summarise, a = mean(a), b = mean(b), c = mean(c), Speed = first(Speed)) %&gt;% map(transmute, si = sqrt(a ^ 2 + b ^ 2) / c, Speed = first(Speed) ) %&gt;% bind_rows() cbss_data &lt;- cbss_df %&gt;% bind_rows(cbss_360) %&gt;% left_join(cbss_vecsum_df, by = &quot;Speed&quot;) %&gt;% left_join(cbss_si_df, by = &quot;Speed&quot;) %&gt;% drop_na(mean_spike_rate) cbss_max_y &lt;- max(cbss_data$mean_spike_rate) bin10_polar_cbss[[i]] &lt;- cbss_data %&gt;% left_join(baseline_summary_df[[i]], by = &quot;Speed&quot;) %&gt;% ggplot(aes(x = Direction, y = mean_spike_rate)) + geom_ribbon(aes( x = Direction, ymin = global_baseline - global_baseline_se, ymax = global_baseline + global_baseline_se ), fill = &quot;darkgoldenrod1&quot;) + stat_smooth(method = &quot;glm&quot;, formula = y ~ ns(x, length(polar_directions[[i]])), linewidth = 0.3, color = &quot;forestgreen&quot;) + geom_point(color = &quot;#333132&quot;) + geom_label(aes(label = round(si, 2) , x = 315, y = cbss_max_y * 1.2), size = 3) + geom_vline(aes(xintercept = vector_sum), colour = &quot;grey30&quot;, size = 0.75) + coord_polar(direction = 1, start = pi/2) + scale_x_continuous( breaks = c(0, 90, 180, 270), expand = c(0, 0), limits = c(0, 360) ) + scale_y_continuous( #trans = &quot;sqrt&quot;, limits = c(0, cbss_max_y * 1.2) ) + facet_grid(cols = vars(Speed)) + ggtitle(&quot;1500-3000 ms (steady state) motion&quot;) + ylab(&quot;Spike rate (spikes/sec)&quot;) + theme_minimal() } ## `summarise()` has grouped output by &#39;Speed&#39;, &#39;Direction&#39;. You can override ## using the `.groups` argument. bin10_polar_cbss[[1]] 7.5 Using 0-500 ms (arbitrary epoch length) ## global baseline values, 0-500 msec motion period bin10_polar_arb &lt;- NULL for (i in 1:length(bin10_filelist)) { arb_df &lt;- bin10_data[[i]] %&gt;% filter(Time_stand &gt;= Static_end + 0.001) %&gt;% filter(Time_stand &lt;= Static_end + 0.500) %&gt;% group_by(Speed, Direction, Replicate) %&gt;% summarize( mean_spike_rate = mean(Spike_rate) ) arb_360 &lt;- arb_df %&gt;% filter(Direction == 0) %&gt;% transmute( Speed = Speed, Direction = 360, Replicate = Replicate, mean_spike_rate = mean_spike_rate) arb_vecsum_df &lt;- arb_df %&gt;% ungroup() %&gt;% drop_na(mean_spike_rate) %&gt;% group_split(Speed) %&gt;% map(group_by, Direction) %&gt;% map(summarize, mean_spike_rate = mean(mean_spike_rate) - unique(baseline_summary_df[[i]]$global_baseline), Speed = first(Speed)) %&gt;% map(mutate, mean_spike_rate = case_when(min(mean_spike_rate) &lt; 0 ~ mean_spike_rate + abs(min(mean_spike_rate)), TRUE ~ mean_spike_rate)) %&gt;% map(transmute, x = cos(Direction * pi / 180) * mean_spike_rate, y = sin(Direction * pi / 180) * mean_spike_rate, Speed = first(Speed) ) %&gt;% map(summarise, x = mean(x), y = mean(y), Speed = first(Speed)) %&gt;% map(transmute, vector_sum = (atan2(y, x) * 180 / pi) %% 360, Speed = first(Speed) ) %&gt;% bind_rows() arb_si_df &lt;- arb_df %&gt;% ungroup() %&gt;% drop_na(mean_spike_rate) %&gt;% group_split(Speed) %&gt;% map(group_by, Direction) %&gt;% map(summarize, mean_spike_rate = mean(mean_spike_rate) - unique(baseline_summary_df[[i]]$global_baseline), Speed = first(Speed)) %&gt;% map(mutate, mean_spike_rate = case_when(min(mean_spike_rate) &lt; 0 ~ mean_spike_rate + abs(min(mean_spike_rate)), TRUE ~ mean_spike_rate)) %&gt;% map(transmute, a = (sin(Direction * pi / 180) * mean_spike_rate), b = (cos(Direction * pi / 180) * mean_spike_rate), c = mean(mean_spike_rate), Speed = first(Speed) ) %&gt;% map(summarise, a = mean(a), b = mean(b), c = mean(c), Speed = first(Speed)) %&gt;% map(transmute, si = sqrt(a ^ 2 + b ^ 2) / c, Speed = first(Speed) ) %&gt;% bind_rows() arb_data &lt;- arb_df %&gt;% bind_rows(arb_360) %&gt;% left_join(arb_vecsum_df, by = &quot;Speed&quot;) %&gt;% left_join(arb_si_df, by = &quot;Speed&quot;) %&gt;% drop_na(mean_spike_rate) arb_max_y &lt;- max(arb_data$mean_spike_rate) bin10_polar_arb[[i]] &lt;- arb_data %&gt;% left_join(baseline_summary_df[[i]], by = &quot;Speed&quot;) %&gt;% ggplot(aes(x = Direction, y = mean_spike_rate)) + geom_ribbon(aes( x = Direction, ymin = global_baseline - global_baseline_se, ymax = global_baseline + global_baseline_se ), fill = &quot;darkgoldenrod1&quot;) + stat_smooth(method = &quot;glm&quot;, formula = y ~ ns(x, length(polar_directions[[i]])), linewidth = 0.3, color = &quot;forestgreen&quot;) + geom_point(color = &quot;#333132&quot;) + geom_label(aes(label = round(si, 2) , x = 315, y = arb_max_y * 1.2), size = 3) + geom_vline(aes(xintercept = vector_sum), colour = &quot;grey30&quot;, size = 0.75) + coord_polar(direction = 1, start = pi/2) + scale_x_continuous( breaks = c(0, 90, 180, 270), expand = c(0, 0), limits = c(0, 360) ) + scale_y_continuous( #trans = &quot;sqrt&quot;, limits = c(0, arb_max_y * 1.2) ) + facet_grid(cols = vars(Speed)) + ylab(&quot;Spike rate (spikes/sec)&quot;) + ggtitle(&quot;0-500 msec motion&quot;) + theme_minimal() } ## `summarise()` has grouped output by &#39;Speed&#39;, &#39;Direction&#39;. You can override ## using the `.groups` argument. bin10_polar_arb[[1]] ## Warning: Removed 17 rows containing missing values (`geom_smooth()`). 7.6 Construct a multi-panel plot We can use the handy cowplot package to construct a multi-panel plot. cow_polar &lt;- NULL for (i in 1:length(bin10_basenames)) { cow_polar[[i]] &lt;- plot_grid(bin10_polar_naive[[i]], bin10_polar_cbit[[i]], bin10_polar_cbss[[i]], bin10_polar_arb[[i]], ncol = 1) } ## Warning: Removed 149 rows containing missing values (`geom_smooth()`). ## Warning: Removed 17 rows containing missing values (`geom_smooth()`). cow_polar[[1]] This plot can also be written directly to PDF. for (i in 1:length(cow_polar)) { pdf(file = paste0(&quot;./plot_pdfs/&quot;, bin10_basenames[i], &quot;_polar_set.pdf&quot;), width = 22, height = 12, pagecentre = TRUE, colormodel = &quot;srgb&quot;) plot(cow_polar[[i]]) dev.off() } "],["spatiotemporal-tuning.html", "8 Spatiotemporal tuning", " 8 Spatiotemporal tuning Coming soon "],["raw-data-and-spike-sorted-traces.html", "9 Raw data and spike sorted traces", " 9 Raw data and spike sorted traces Coming soon "],["histological-verification.html", "10 Histological verification", " 10 Histological verification Test post, please ignore 10.0.1 Direct embedding method: Flowchart 10.0.2 Hotlinking method: Flowchart link "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
